version: "3"
tasks:
  __ssh-command:
    silent: true
    internal: true
    vars:
      MASTER_NAME:
        sh: kubectl -n {{ .NAMESPACE }} get vm -l dvp.deckhouse.io/node-group=master -o jsonpath="{.items[0].metadata.name}"
    cmds:
      - /usr/bin/ssh -i {{ .SSH_PRIV_KEY_FILE }} -o LogLevel=ERROR -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o 'ProxyCommand=d8 v port-forward --stdio=true {{ .MASTER_NAME }}.{{ .NAMESPACE }} 22' ubuntu@{{ .MASTER_NAME }} {{ .CMD }}

  deploy:
    silent: true
    desc: Deploy VM linux-vm-001
    cmds:
    - task: __ssh-command
      vars:
        CMD: |
          sudo /opt/deckhouse/bin/kubectl apply -f - <<EOF
          $(cat ./test/vm.yaml)
          EOF

  undeploy:
    silent: true
    desc: Undeploy VM linux-vm-001
    cmds:
    - task: __ssh-command
      vars:
        CMD: |
          sudo /opt/deckhouse/bin/kubectl delete -f - <<EOF
          $(cat ./test/vm.yaml)
          EOF

  evict:
    silent: true
    desc: Evict VM linux-vm-001
    cmds:
    - task: __ssh-command
      vars:
        CMD: |
          sudo /opt/deckhouse/bin/d8 v -n vms evict linux-vm-001

  con:
    silent: true
    desc: Connect to VM linux-vm-001 via console
    cmds:
    - task: __ssh-command
      vars:
        CMD: |
          sudo /opt/deckhouse/bin/d8 v -n vms console linux-vm-001

  check:
    silent: true
    desc: Check VM linux-vm-001 state
    cmds:
    - task: __ssh-command
      vars:
        CMD: |
          sudo /opt/deckhouse/bin/kubectl -n vms get virtualization -o wide

  nested:kubeconfig:via-32443:
    vars:
      DOMAIN:
        sh: |
          set -e
          export KUBECONFIG="{{ .PARENT_KUBECONFIG }}"
          kubectl get mc global -o jsonpath='{.spec.settings.modules.publicDomainTemplate}' | sed 's/%s\.//'
      RUN_ID:
        sh: |
          set -e
          export KUBECONFIG="{{ .PARENT_KUBECONFIG }}"
          # Try to derive from nested Ingress host (api-<runid>.<domain>) quietly
          HOST="$(kubectl -n {{ .NAMESPACE }} get ingress nested-kubeapi -o jsonpath='{.spec.rules[0].host}' 2>/dev/null || true)"
          if [ -n "$HOST" ] && echo "$HOST" | grep -q '^api-'; then
            echo "$HOST" | sed 's/^api-\(.*\)\..*$/\1/' | tr -d '\n'
            exit 0
          fi
          # Fallback: extract suffix from master VM name e.g. demo-cluster-master-0-518d29 -> 518d29
          VM_NAME="$(kubectl -n {{ .NAMESPACE }} get vm -l dvp.deckhouse.io/node-group=master -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
          if [ -n "$VM_NAME" ]; then
            echo "$VM_NAME" | sed -E 's/.*-([a-f0-9]{5,8})$/\1/' | tr -d '\n'
            exit 0
          fi
          # Nothing worked: emit truly empty (no newline)
          printf %s ""
      OUT: tmp/nested/{{ .NAMESPACE }}-sa-32443.kubeconfig
    cmds:
      - |
        set -euo pipefail
        if [ -z "$(printf %s "{{ .RUN_ID }}")" ]; then
          echo "[ERR] cannot determine RUN_ID (no nested-kubeapi ingress and no master VM found)"; exit 1;
        fi

        if [ ! -s tmp/nested/sa-token.txt ]; then
          echo "[ERR] tmp/nested/sa-token.txt not found; run a task that issues SA token first (e.g. nested:get-kubeconfig or nested:sa:bootstrap-token)"; exit 1;
        fi

        TOKEN="$(cat tmp/nested/sa-token.txt)"
        API_URL="https://api-{{ .RUN_ID }}.{{ .DOMAIN }}:32443"
        OUT_FILE="{{ .OUT }}"
        mkdir -p tmp/nested

        : > "$OUT_FILE"
        printf '%s\n' 'apiVersion: v1' >> "$OUT_FILE"
        printf '%s\n' 'kind: Config' >> "$OUT_FILE"
        printf '%s\n' 'clusters:' >> "$OUT_FILE"
        printf '%s\n' '- cluster:' >> "$OUT_FILE"
        printf '    server: %s\n' "$API_URL" >> "$OUT_FILE"
        printf '%s\n' '    insecure-skip-tls-verify: true' >> "$OUT_FILE"
        printf '%s\n' '  name: nested' >> "$OUT_FILE"
        printf '%s\n' 'contexts:' >> "$OUT_FILE"
        printf '%s\n' '- context:' >> "$OUT_FILE"
        printf '%s\n' '    cluster: nested' >> "$OUT_FILE"
        printf '%s\n' '    user: sa' >> "$OUT_FILE"
        printf '%s\n' '  name: nested' >> "$OUT_FILE"
        printf '%s\n' 'current-context: nested' >> "$OUT_FILE"
        printf '%s\n' 'users:' >> "$OUT_FILE"
        printf '%s\n' '- name: sa' >> "$OUT_FILE"
        printf '  user:\n    token: "%s"\n' "$TOKEN" >> "$OUT_FILE"
        chmod 600 "$OUT_FILE"
        echo "[OK] wrote $OUT_FILE (server: $API_URL)"

        # quick checks
        KUBECONFIG="$OUT_FILE" kubectl get --raw=/version | head -c 200 || true
        KUBECONFIG="$OUT_FILE" kubectl get nodes -o wide || true

  nested:sa:bootstrap-token:
    desc: "Создать временный SA-токен (10d) внутри nested через SSH на мастер (без локального admin.conf)"
    vars:
      PARENT_KUBECONFIG: '{{ .PARENT_KUBECONFIG | default "" }}'
      DEFAULT_USER: '{{ .DEFAULT_USER | default "ubuntu" }}'
      MASTER_NAME:
        sh: |
          set -e
          export KUBECONFIG="{{ .PARENT_KUBECONFIG }}"
          kubectl -n {{ .NAMESPACE }} get vm -l dvp.deckhouse.io/node-group=master -o jsonpath='{.items[0].metadata.name}'
    cmds:
      - mkdir -p tmp/nested
      - |
        set -euo pipefail
        if [ -z "{{ .PARENT_KUBECONFIG }}" ]; then
          echo "[ERR] PARENT_KUBECONFIG is empty. Run: export PARENT_KUBECONFIG=$(cat tmp/parent-kubeconfig.path)"; exit 1;
        fi
        if [ -z "{{ .MASTER_NAME }}" ]; then
          echo "[ERR] Master VM not found in namespace '{{ .NAMESPACE }}'"; exit 1;
        fi
        echo "[INFO] ensuring ServiceAccount/CRB and issuing token on nested (remote kubectl)…"
        ~/d8 v ssh --username={{ .DEFAULT_USER }} --identity-file=./tmp/ssh/cloud --local-ssh=true --local-ssh-opts="-o StrictHostKeyChecking=no" --local-ssh-opts="-o UserKnownHostsFile=/dev/null" {{ .MASTER_NAME }}.{{ .NAMESPACE }} -c '
          set -euo pipefail
          SUDO="sudo /opt/deckhouse/bin/kubectl"
          # Ensure SA exists (idempotent)
          $SUDO -n kube-system create serviceaccount e2e-runner --dry-run=client -o yaml | $SUDO apply -f -
          # Ensure CRB exists (idempotent)
          $SUDO create clusterrolebinding e2e-runner-admin \
            --clusterrole=cluster-admin \
            --serviceaccount=kube-system:e2e-runner \
            --dry-run=client -o yaml | $SUDO apply -f -
          # Issue token for 10 days
          $SUDO -n kube-system create token e2e-runner --duration=240h
        ' > tmp/nested/sa-token.txt
        head -c 20 tmp/nested/sa-token.txt >/dev/null
        echo "[OK] token saved to tmp/nested/sa-token.txt"

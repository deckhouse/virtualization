version: "3"

silent: true

includes:
  init: Taskfile.init.yaml
  cvmi: Taskfile.cvmi.yaml
  vmi: Taskfile.vmi.yaml
  my:
    taskfile: Taskfile.my.yaml
    optional: true
  dvcr:
    taskfile: ../dvcr-artifact/Taskfile.dist.yaml
    dir: ../dvcr-artifact/

vars:
  CDIVersion: "v1.57.0-alpha1"
  KubeVirtVersion: "v1.0.0"
  CaddyVersion: "2.4.5"
  LocalRegistry: "k3d-registry.virtualization-controller.test:5000"
  DevRegistry: "dev-registry.deckhouse.io/virt/dev"
  ControllerImageName: "virtualization-controller:latest"
  ControllerBuilderImageName: "virtualization-controller-builder:latest"
  ImporterImageName: "virtualization-importer:latest"
  UploaderImageName: "virtualization-uploader:latest"
  ClusterType:
    sh: |
      # Cluster type may be overriden with CLUSTER_TYPE env var.
      [ -n "${CLUSTER_TYPE}" ] && echo -n ${CLUSTER_TYPE} && exit
      # Cluster type detection is a heavy operation. Run kubectl only for particular tasks.
      ForTasks="build:controller build:importer build:uploader dev:up dev:run caddy:up dvcr:up cluster:info _ensure:cluster-type _ensure:image-prefix"
      for task in $ForTasks ; do
        if [[ "{{.TASK}}" == "${task}" || "{{.TASK}}" == *":${task}" ]] ; then
          if nodes=$(kubectl get no -o wide --show-labels) ; then
            echo "${nodes}" | grep node.kubernetes.io/instance-type=k3s 2>&1 >/dev/null \
              && echo "local" || echo "remote"
            break
          fi
        fi
      done

  ImagePrefix:
    desc: 'This prefix is required to use one shared registry'
    sh: |
      # Image prefix may be overriden with IMAGE_PREFIX env var.
      [ -n "${IMAGE_PREFIX}" ] && echo -n ${IMAGE_PREFIX} && exit
      # Image prefix detection is a heavy operation. Run it only for particular tasks.
      ForTasks="build:controller build:importer build:uploader dev:run cluster:info _ensure:image-prefix"
      for task in $ForTasks ; do
        if [[ "{{.TASK}}" == "${task}" || "{{.TASK}}" == *":${task}" ]] ; then
          kubectl get no -o jsonpath='{.items[0].metadata.name}' -l node-role.kubernetes.io/master= 2>/dev/null || echo ""
          break
        fi
      done
env:
  K8S_CODEGEN:
    sh: echo $(go env GOMODCACHE)/$(go list -f '{{`{{.Path}}@{{.Version}}`}}' -m k8s.io/code-generator)
  CONTROLLER_IMAGE:
    sh: echo ${CONTROLLER_IMAGE:-k3d-registry.virtualization-controller.test:5000/{{.ControllerImageName}}}
  CONTROLLER_BUILDER_IMAGE:
    sh: echo ${CONTROLLER_BUILDER_IMAGE:-k3d-registry.virtualization-controller.test:5000/{{.ControllerBuilderImageName}}}
  CONTROLLER_TEST_OS_IMAGES:
    sh: echo ${CONTROLLER_TEST_OS_IMAGES:-dev-registry.deckhouse.io/virt/test-os-images:latest}
tasks:
  dev:converge:
    desc: "Rebuild and deploy all components into detected cluster"
    cmds:
      - task: build
      - task: dev:run

  dev:reset:
    desc: "Reset local kubernetes cluster"
    cmds:
      - task: dev:down
      - task: dev:up

  dev:up:
    desc: "Setup required components in local k3d cluster"
    deps:
      - _ensure:k3d
      - _ensure:crane
      - _ensure:kubectl
      - _ensure:virtctl
      - _ensure:k3d-registry-domain
      - _ensure:k3d-registry-insecure-allowed
    vars:
      ClusterType: "local"
    env:
      CLUSTER_TYPE: "local"
    cmds:
      - task: cluster:up

      - task: dev:preload-images

      - task: kubevirt:up

      - task: cdi:up

      - task: caddy:up
      - task: caddy:preload-os-images

      - task: dvcr:up

  dev:down:
    desc: "Delete local k3d cluster"
    deps:
      - _ensure:k3d
      - _ensure:kubectl
      - _ensure:k3d-registry-domain
      - _ensure:k3d-registry-insecure-allowed
      - _ensure:image-prefix
    cmds:
      - task: cluster:down
      - task: build:cache:reset

  dev:up:remote:
    desc: "Setup components in a remote deckhouse cluster"
    vars:
      ClusterType: "remote"
    cmds:
      - task: _ensure:cluster-available
      - task: caddy:up
      - task: dvcr:up

  dev:preload-images:
    desc: "Preload images for all components"
    cmds:
      - task: cdi:preload-images
      - task: kubevirt:preload-images
      - task: caddy:preload-images
      - task: dvcr:preload-images

  dev:rmns:
    desc: "Remove namespace with finalizer"
    cmds:
      - |
        ns=virtualization-controller ; kubectl get namespace $ns -o json | jq '.spec.finalizers= []' | kubectl replace --raw "/api/v1/namespaces/$ns/finalize" -f -

  cluster:up:
    desc: "Create local k3d cluster"
    cmds:
      - ( cd $HOME/.virtualization-controller-k3d ; mkdir -p mounts storage )
      - k3d registry create registry.virtualization-controller.test --port 5000
      - |
        kubevirt_mounts=()
        if [[ "{{OS}}" == "linux" ]] ; then
          kubevirt_mounts=(
            --volume "$HOME/.virtualization-controller-k3d/mounts/var/run/kubevirt:/var/run/kubevirt"
            --volume "$HOME/.virtualization-controller-k3d/mounts/var/run/kubevirt-private:/var/run/kubevirt-private"
          )
        fi
        k3d cluster create \
            --api-port 6550 -p "80:80@loadbalancer" \
            --registry-use k3d-registry.virtualization-controller.test:5000 \
            --volume $HOME/.virtualization-controller-k3d/storage:/var/lib/rancher/k3s/storage@all \
            "${kubevirt_mounts[@]}"

  cluster:info:
    desc: "Show cluster type and installed components"
    cmds:
      - |
        echo "Will use context '$(kubectl config current-context)' for kubectl and helm"
      - task: _ensure:image-prefix
      - |
        echo 'Detected cluster type is {{.ClusterType}}'
        if [ "{{.ClusterType}}" == "remote" ] ; then
          echo 'Detected image prefix is {{.ImagePrefix}}'
          echo Dev images:
          echo '  Controller: {{.DevRegistry}}/{{.ImagePrefix}}/{{ .ControllerImageName }}'
          echo '  Importer: {{.DevRegistry}}/{{.ImagePrefix}}/{{ .ImporterImageName }}'
          echo '  Uploader: {{.DevRegistry}}/{{.ImagePrefix}}/{{ .UploaderImageName }}'
        fi



  cluster:down:
    desc: ""
    cmds:
      - k3d registry delete --all || true
      - k3d cluster delete || true

  kubevirt:preload-images:
    desc: "Preload images for Kubevirt components"
    cmds:
      - |
        # Preload kubevirt images.
        docker pull quay.io/kubevirt/virt-api:{{.KubeVirtVersion}}
        k3d image import quay.io/kubevirt/virt-api:{{.KubeVirtVersion}}
        docker pull quay.io/kubevirt/virt-controller:{{.KubeVirtVersion}}
        k3d image import quay.io/kubevirt/virt-controller:{{.KubeVirtVersion}}
        docker pull quay.io/kubevirt/virt-handler:{{.KubeVirtVersion}}
        k3d image import quay.io/kubevirt/virt-handler:{{.KubeVirtVersion}}
        docker pull quay.io/kubevirt/virt-launcher:{{.KubeVirtVersion}}
        k3d image import quay.io/kubevirt/virt-launcher:{{.KubeVirtVersion}}
        docker pull quay.io/kubevirt/virt-operator:{{.KubeVirtVersion}}
        k3d image import quay.io/kubevirt/virt-operator:{{.KubeVirtVersion}}

  kubevirt:up:
    desc: "Insall kubevirt into local cluster"
    cmds:
      - |
        # Install KubeVirt
        export RELEASE={{.KubeVirtVersion}}
        kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml
        kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-cr.yaml

        if [[ "{{OS}}" == "darwin" ]] ; then
          # Enable emulation mode for MacOS.
          echo "Enable Software emulation ..."
          kubectl -n kubevirt patch kubevirt kubevirt  --type=merge --patch '{"spec": {"configuration": {"developerConfiguration": {"useEmulation": true}}}}'
        else
          # Do not wait for kubevirt on MacOS, it is always stuck in the CreateContainerError phase.
          echo "Wait until kubevirt is ready ..."
          kubectl -n kubevirt wait kv kubevirt --for condition=Available --timeout=5m
        fi


  cdi:preload-images:
    desc: "Preload images for cdi-importer components"
    cmds:
      - |
        # Preload images for faster start of cdi-importer.
        docker pull       quay.io/kubevirt/cdi-importer:{{.CDIVersion}}
        k3d image import  quay.io/kubevirt/cdi-importer:{{.CDIVersion}}
        docker pull       quay.io/kubevirt/cdi-operator:{{.CDIVersion}}
        k3d image import  quay.io/kubevirt/cdi-operator:{{.CDIVersion}}
        docker pull       quay.io/kubevirt/cdi-apiserver:{{.CDIVersion}}
        k3d image import  quay.io/kubevirt/cdi-apiserver:{{.CDIVersion}}
        docker pull       quay.io/kubevirt/cdi-uploadproxy:{{.CDIVersion}}
        k3d image import  quay.io/kubevirt/cdi-uploadproxy:{{.CDIVersion}}

  cdi:up:
    desc: "Install CDI into local cluster"
    cmds:
      - |
        # Install CDI
        export TAG=$(curl -s -w %{redirect_url} https://github.com/kubevirt/containerized-data-importer/releases/latest)
        export VERSION=$(echo ${TAG##*/})
        kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-operator.yaml
        kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-cr.yaml

      - |
        kubectl patch cdi cdi \
            --patch '{"spec": {"config": {"insecureRegistries": ["k3d-registry.virtualization-controller.test:5000"]}}}' \
            --type merge

  caddy:up:
    desc: "Start caddy file server"
    cmds:
      - |
        echo "Install caddy server into '{{.ClusterType}}' cluster ..."

        helm upgrade --install \
          --namespace caddy \
          --create-namespace \
          --set clusterType={{.ClusterType}} \
          caddy ./local/caddy
      - task: _copy_d8_registry_secret
        vars:
          NAMESPACE: caddy

  caddy:down:
    desc: "Uninstall caddy"
    cmds:
      - |
        helm uninstall --namespace caddy caddy

  caddy:preload-os-images:force:
    desc: "Force download and extract OS images for local HTTP server"
    cmds:
      - |
        echo "Cleanup caddy-images storage ..."
        rm -rf $HOME/.virtualization-controller-k3d/storage/caddy-images/*
      - task: caddy:preload-os-images

  caddy:preload-os-images:
    desc: "Preload cloud images that will be http-served in the local kubernetes cluster"
    deps:
      - task: _ensure:crane
    cmds:
      - |
        CADDY_DIR="$HOME/.virtualization-controller-k3d/storage/caddy-images"
        if [ "$(ls -A $CADDY_DIR)" ] ; then
          echo "Images already exist, skip downloading. Use 'task caddy:preload-os-images:force' to force downloading."
          exit 0
        fi
        echo "Download and extract test OS images (be patient) ..."
        # Extract *.img and *.iso files from archive in dev-registry.
        # $HOME/.virtualization-controller-k3d/storage/caddy-images is mounted as caddy Pod volume
        crane export ${CONTROLLER_TEST_OS_IMAGES} - | tar -C $HOME/.virtualization-controller-k3d/storage/caddy-images --strip-components=4 -xvf - usr/share/caddy/images

  caddy:push-os-images:
    desc: "Archive and push cloud images that will be http-served in the local kubernetes cluster"
    cmds:
      - |
        [ -f local/caddy/images/ubuntu-22.04-minimal-cloudimg-amd64.img ] || wget https://cloud-images.ubuntu.com/minimal/releases/jammy/release-20230615/ubuntu-22.04-minimal-cloudimg-amd64.img -O local/caddy/images/ubuntu-22.04-minimal-cloudimg-amd64.img
      - |
        docker build ./local/caddy/ \
          -f ./local/caddy/Dockerfile \
          -t ${CONTROLLER_TEST_OS_IMAGES} \
          --platform linux/amd64
         docker push ${CONTROLLER_TEST_OS_IMAGES}

  caddy:preload-images:
    desc: "Preload images for Caddy components."
    cmds:
      - |
        # Preload images for Caddy components.
        docker pull "caddy:{{.CaddyVersion}}"
        k3d image import "caddy:{{.CaddyVersion}}"

  dvcr:preload-images:
    desc: "Preload images for DVCR components."
    cmds:
      - |
        echo "Import images for DVCR ..."
        docker pull nginx:stable-alpine
        k3d image import nginx:stable-alpine
        docker pull registry:2
        k3d image import registry:2

  dvcr:up:
    desc: "Install dvcr registry into the cluster using helm"
    deps:
      - _ensure:helm
      - _ensure:kubectl
      - _ensure:cluster-type
    cmds:
      - |
        # Gen certs.
        if [[ ! -f ./local/dvcr/certs/ca.csr ]] ; then
          echo "Generate TLS certificates for DVCR ..."
          cd ./local/dvcr
          ./gen-certs.sh
        fi
      - |
        helm upgrade --install \
          --namespace dvcr \
          --create-namespace \
          --set clusterType={{.ClusterType}} \
          dvcr ./local/dvcr

  dvcr:down:
    desc: "Remove DVCR from cluster"
    cmds:
      - |
        helm uninstall --namespace dvcr dvcr
      - |
        kubectl delete ns/dvcr

  build:cache:reset:
    desc: "Reset go build cache"
    cmds:
      - |
        # Bootstrap builder image (only needed for build with BUILDER_CACHE_IMAGE)
        docker rmi -f $CONTROLLER_BUILDER_IMAGE || true

  build:
    cmds:
      - task: build:controller
      - task: build:importer
      - task: build:uploader
      - task: build:cleanup

  build:controller:nocache:
    desc: "Build virtualization-controller image"
    cmds:
      # Simple build without BUILDER_CACHE_IMAGE build-arg (slower, but simple).
      - |
        docker build . \
            -f ./local/virtualization-controller/Dockerfile \
            -t ${CONTROLLER_IMAGE} \
            --platform linux/amd64
        docker push ${CONTROLLER_IMAGE}

  build:controller:
    desc: "Build virtualization-controller image for local development"
    cmds:
      - task: _ensure:image-prefix
      - task: _build:controller
        vars:
          RemoteImage:
            sh: '[ "{{.ClusterType }}" == "remote" ] && echo "{{.DevRegistry}}/{{.ImagePrefix}}/{{.ControllerImageName}}" || true'

  _build:controller:
    internal: true
    vars:
      ControllerImage: '{{.RemoteImage | default "${CONTROLLER_IMAGE}" }}'
    deps:
      - task: _build:controller:cache
    cmds:
      # Build with BUILDER_CACHE_IMAGE build-arg (faster, but needs more space)
      - |
        IMAGE="{{.ControllerImage}}"
        echo "Build virtualization-controller image $IMAGE ..."
        docker build . \
            -f ./local/virtualization-controller/Dockerfile \
            --build-arg BUILDER_CACHE_IMAGE=${CONTROLLER_BUILDER_IMAGE} \
            -t ${IMAGE} \
            --platform linux/amd64
        docker push ${IMAGE}

  _build:controller:cache:
    internal: true
    desc: "Download Go modules and prebuild them to speedup next builds of the controller."
    cmds:
      - |
        BUILDER_IMAGE="{{.BuilderImage}}"
        docker build . \
            -f ./local/virtualization-controller/Dockerfile \
            --target builder \
            -t ${CONTROLLER_BUILDER_IMAGE} \
            --platform linux/amd64
    status:
      - |
        docker image inspect ${CONTROLLER_BUILDER_IMAGE} -f '{{ .Created }}'

  build:importer:
    desc: "Build dvcr-importer image for local development"
    cmds:
      - task: _ensure:image-prefix
      - task: dvcr:importer:push
        vars:
          IMAGE_NAME: '{{.ImporterImageName}}'
          REGISTRY:
            sh: '[ "{{.ClusterType }}" == "remote" ] && echo "{{.DevRegistry}}/{{.ImagePrefix}}" || true'

  build:uploader:
    desc: "Build dvcr-uploader image for local development"
    cmds:
      - task: _ensure:image-prefix
      - task: dvcr:uploader:push
        vars:
          IMAGE_NAME: '{{.UploaderImageName}}'
          REGISTRY:
            sh: '[ "{{.ClusterType }}" == "remote" ] && echo "{{.DevRegistry}}/{{.ImagePrefix}}" || true'

  build:cleanup:
    cmds:
      - |
        echo "Remove dangling images"
        docker rmi -f $(docker image ls | grep k3d-registry.virtualization-controller.test:5000/ | grep '<none>' | cut -w -f 3) 2>/dev/null || true
        docker rmi -f $(docker image ls | grep {{ .DevRegistry }}/ | grep '<none>' | cut -w -f 3) 2>/dev/null || true

  build:go:apiserver:
    env:
      PKG: "k8s.io/client-go/pkg"
      BINARY_NAME: "virtualization-api"
      CGO_ENABLED: "0"
      GOOS: "linux"
      GOARCH: "amd64"
      GIT_COMMIT:
        sh: git rev-parse "HEAD^{commit}" 2>/dev/null
      GIT_TAG:
        sh: git describe --abbrev=0 --tags 2>/dev/null
      BUILD_DATE:
        sh: date -u +'%Y-%m-%dT%H:%M:%SZ'
    cmds:
      - |
        go build -mod=readonly -trimpath \
        -ldflags "-w -X $PKG/version.gitVersion=$GIT_TAG -X $PKG/version.gitCommit=$GIT_COMMIT -X $PKG/version.buildDate=$BUILD_DATE" \
        -o "$PWD/bin/$BINARY_NAME" $PWD/cmd/virtualization-api

  dev:run:
    desc: "Install virtualization-controller into the cluster using helm"
    deps:
      - _ensure:helm
      - _ensure:kubectl
      - _ensure:cluster-available
      - _ensure:image-prefix
    preconditions:
      - sh: |
          docker images | grep 'virtualization-controller.*latest'
        msg: 'Image for virtualization-controller should be built'
    cmds:
      - |
        mkdir -p local/virtualization-controller/crds
        cp api/crds/*.yaml local/virtualization-controller/crds
      - (cd local/virtualization-controller && bash gen-certs.sh -y)
      - |
        images=()
        if [ "{{.ClusterType}}" == "remote" ] ; then
          images=(
            --set images.controller={{.DevRegistry}}/{{.ImagePrefix}}/{{ .ControllerImageName }}
            --set images.importer={{.DevRegistry}}/{{.ImagePrefix}}/{{ .ImporterImageName }}
            --set images.uploader={{.DevRegistry}}/{{.ImagePrefix}}/{{ .UploaderImageName }}
          )
        fi
        helm upgrade --install \
          --namespace virtualization-controller \
          --create-namespace \
          "${images[@]}" \
          virtualization-controller ./local/virtualization-controller
      - task: _copy_d8_registry_secret
        vars:
          NAMESPACE: virtualization-controller

  dev:logs:
    desc: "Watch current virtualization-controller logs"
    cmds:
      - kubectl -n virtualization-controller logs deploy/virtualization-controller -f

  dev:delete:
    desc: "Delete virtualization-controller from the cluster using helm"
    deps:
      - _ensure:helm
      - _ensure:kubectl
      - _ensure:cluster-available
    cmds:
      - |
        helm delete \
          --namespace virtualization-controller \
          virtualization-controller
      - |
        kubectl delete ns/virtualization-controller

  dev:update:crds:
    desc: "Apply CRD manifests from api directory"
    cmds:
      - task: dev:update:cvmi
      - task: dev:update:vmi
      - task: dev:update:vmd
      - task: dev:update:vmds
      - task: dev:update:vm
      - task: dev:update:vmbda
      - task: dev:update:vmip
      - task: dev:update:vmipl

  dev:update:cvmi:
    desc: "Update ClusterVirtualMachineImage CRD"
    cmds:
      - kubectl apply -f api/crds/clustervirtualmachineimage.yaml

  dev:update:vmi:
    desc: "Update VirtualMachineImage CRD"
    cmds:
      - kubectl apply -f api/crds/virtualmachineimage.yaml

  dev:update:vmd:
    desc: "Update VirtualMachineDisk CRD"
    cmds:
      - kubectl apply -f api/crds/virtualmachinedisk.yaml

  dev:update:vmds:
    desc: "Update VirtualMachineDiskSnapshot CRD"
    cmds:
      - kubectl apply -f api/crds/virtualmachinedisksnapshot.yaml

  dev:update:vm:
    desc: "Update VirtualMachine CRD"
    cmds:
      - kubectl apply -f api/crds/virtualmachine.yaml

  dev:update:vmbda:
    desc: "Update VirtualMachineBlockDeviceAttachment CRD"
    cmds:
      - kubectl apply -f api/crds/virtualmachineblockdeviceattachment.yaml

  dev:update:vmip:
    desc: "Update VirtualMachineIPAddressClaim CRD"
    cmds:
      - kubectl apply -f api/crds/virtualmachineipaddressclaim.yaml

  dev:update:vmipl:
    desc: "Update VirtualMachineIPAddressLease CRD"
    cmds:
      - kubectl apply -f api/crds/virtualmachineipaddresslease.yaml

  dev:update:vmop:
    desc: "Update VirtualMachineOperation CRD"
    cmds:
      - kubectl apply -f api/crds/virtualmachineoperations.yaml

  kctl:
    cmds:
      - kubectl -n virtualization-controller {{.CLI_ARGS}}
  generate:
    desc: "Regenerate all"
    deps:
      - _ensure:k8s-codegen-mod
      - _ensure:k8s-kube-openapi
    cmd:  ./hack/update-codegen.sh all
  generate:core:
    desc: "Regenerate code for core components."
    deps:
      - _ensure:k8s-codegen-mod
    cmd: ./hack/update-codegen.sh core
  generate:subresources:
    desc: "Regenerate code for subresources components"
    deps:
      - _ensure:k8s-codegen-mod
      - _ensure:k8s-kube-openapi
    cmd: ./hack/update-codegen.sh subresources
  ci:generate:
    desc: "Run generations and check git diff to ensure all files are committed"
    cmds:
      - task: generate
      - task: _ci:verify-gen

  _ci:verify-gen:
    desc: "Check generated files are up-to-date."
    internal: true
    cmds:
      - |
        git diff --exit-code || (echo "Please run task gen:api and commit changes" && exit 1)

  fmt:
    desc: "Run formatters locally"
    cmds:
      - task: fmt:gci
      - task: fmt:gofumpt

  fmt:gci:
    desc: "Format code with gci, important vars: paths."
    cmds:
      - |
        gci write --skip-generated -s standard,default,prefix\(github.com/deckhouse/\) {{.CLI_ARGS}} {{.paths | default "pkg/ cmd/"}}

  fmt:gofumpt:
    desc: "Format code with gofumpt, important vars: paths"
    cmds:
      - |
        gofumpt -extra -w {{.CLI_ARGS}} {{.paths | default "cmd/ pkg/"}}

  test:unit:
    desc: "Run go unit tests"
    cmds:
      - |
        ginkgo -v -r pkg/

  lint:
    desc: "Run linters locally"
    cmds:
      - task: lint:go

  lint:go:
    desc: "Run golangci-lint"
    deps:
      - _ensure:golangci-lint
    cmds:
      - |
        golangci-lint run --sort-results

  mirrord:run:controller:
    desc: "Run local virtualization-controller in cluster using a mirrord"
    deps:
      - _ensure:mirrord
    cmd: |
        ./hack/mirrord.sh run --app=$PWD/cmd/virtualization-controller/main.go \
        --deployment=virtualization-controller                                 \
        --namespace=d8-virtualization
  mirrord:wipe:controller:
    desc: "wipe up Mirrord's trash"
    deps:
      - _ensure:mirrord
    cmd: ./hack/mirrord.sh wipe --deployment=virtualization-controller --namespace=d8-virtualization
  mirrord:run:apiserver:
    desc: "Run local virtualization-api in cluster using a mirrord"
    deps:
      - _ensure:mirrord
    cmd: |
        flags=()
        flags+=( "--kubevirt-cabundle=/etc/virt-api/certificates/ca.crt" )
        flags+=( "--kubevirt-endpoint=virt-api.d8-virtualization.svc" )
        flags+=( "--secure-port=8443" )
        flags+=( "--tls-private-key-file=/etc/virtualziation-api/certificates/tls.key" )
        flags+=( "--tls-cert-file=/etc/virtualziation-api/certificates/tls.crt" )
        flags+=( "--v=7" )
        flags+=( "--proxy-client-cert-file=/etc/virtualziation-api-proxy/certificates/tls.crt" )
        flags+=( "--proxy-client-key-file=/etc/virtualziation-api-proxy/certificates/tls.key" )
        flags+=( "--service-account-name=virtualization-api" )
        flags+=( "--service-account-namespace=d8-virtualization" )

        ./hack/mirrord.sh run --app="$PWD/cmd/virtualization-api/main.go" \
        --deployment="virtualization-api"                                 \
        --namespace="d8-virtualization"                                   \
        --flags="\"${flags[@]}\""

  mirrord:wipe:apiserver:
    desc: "wipe up Mirrord's trash"
    deps:
      - _ensure:mirrord
    cmd: ./hack/mirrord.sh wipe --deployment=virtualization-api --namespace=d8-virtualization

  _copy_d8_registry_secret:
    internal: true
    vars:
      NAMESPACE: '{{.NAMESPACE}}'
    cmds:
      - |
        [ "{{.NAMESPACE}}" == "" ] && echo "No NS specified for copy deckhouse-registry Secret." && exit 0
        # Copy d8 registry secret
        if kubectl get -n kube-system secret deckhouse-registry ; then
        kubectl get -n kube-system secret deckhouse-registry -o json \
        | jq 'del(.metadata["namespace","creationTimestamp","resourceVersion","selfLink","uid"])' \
        | kubectl apply -n "{{.NAMESPACE}}" -f -
        fi

  _ensure:k8s-codegen-mod:
    desc: "Ensure k8s.io/codegen module is present"
    internal: true
    cmds:
      - echo -e "Path K8S_CODEGEN=${K8S_CODEGEN} should exists.\nAdd k8s.io/code-generator to go.mod and run go mod download" > /dev/stderr
      - exit 1
    status:
      - |
        ls $K8S_CODEGEN && find $K8S_CODEGEN -name \*.sh -exec chmod +x {} \;

  _ensure:k8s-kube-openapi:
    desc: "Ensure k8s.io/kube-openapi"
    internal: true
    cmds:
      - go install -mod=readonly k8s.io/kube-openapi/cmd/openapi-gen
    status:
      - |
        ls $GOPATH/bin/openapi-gen
  _ensure:helm:
    desc: "Ensure helm is installed"
    internal: true
    cmds:
      - echo "Install helm" && exit 1
    status:
      - which helm >/dev/null

  _ensure:kind:
    desc: "Ensure kind is installed"
    internal: true
    cmds:
      - echo "Install kind" && exit 1
    status:
      - which kind >/dev/null

  _ensure:kubectl:
    desc: "Ensure kubectl is installed"
    internal: true
    cmds:
      - echo "Install kubectl" && exit 1
    status:
      - which kubectl >/dev/null

  _ensure:cluster-available:
    desc: "Ensure kubernetes cluster available"
    internal: true
    preconditions:
      - sh: kubectl version
      #>/dev/null
        msg: |-
          No cluster available, run 'task dev:up' or 'task dev:cluster:reset' to setup local k3d cluster
          or switch KUBECONFIG to use remote cluster.

  _ensure:cluster-type:
    desc: "Ensure cluster type variable is not empty"
    internal: true
    preconditions:
      - sh: '[ -n "{{.ClusterType}}" ]'
        msg: |-
          ClusterType is empty. Check cluster connection or set CLUSTER_TYPE env var.
#      - sh: '[[ "{{.ClusterType}}" != "remote" || -n "{{.ImagePrefix}}" ]]'
#        msg: |-
#          Image prefix not detected for remote cluster. Switch KUBECONFIG, check SSH tunnel, or set .

  _ensure:image-prefix:
    internal: true
    run: once
    preconditions:
      - sh: '[ -n "{{.ClusterType}}" ]'
        msg: |-
          ClusterType is empty. Check cluster connection or set CLUSTER_TYPE env var.
      - sh: '[[ "{{.ClusterType}}" != "remote" || -n "{{.ImagePrefix}}" ]]'
        msg: |-
          ImagePrefix is empty: '{{.ImagePrefix}}'. Check cluster connection or set IMAGE_PREFIX env var.

  _ensure:k3d:
    desc: "Ensure k3d is installed"
    internal: true
    cmds:
      - echo "Install k3d" && exit 1
    status:
      - which k3d >/dev/null

  _ensure:crane:
    desc: "Ensure crane is installed"
    internal: true
    cmds:
      - echo "Install crane" && exit 1
    status:
      - which crane >/dev/null

  _ensure:k3d-registry-domain:
    desc: "Ensure k3d registry domain available locally"
    internal: true
    cmds:
      - grep 'k3d-registry.virtualization-controller.test' /etc/hosts || (echo >&2 "Please add '127.0.0.1   k3d-registry.virtualization-controller.test' into /etc/hosts" && exit 1)

  _ensure:k3d-registry-insecure-allowed:
    desc: "Ensure docker daemon allows insecure k3d registry"
    internal: true
    cmds:
      - |
        echo -e >&2 "Please add k3d registry as insecure:\n" \
          '{"insecure-registries": ["k3d-registry.virtualization-controller.test:5000"]'"}\n" \
          "On Linux: edit /etc/docker/daemon.json and restart docker daemon with 'sudo systemctl restart docker'\n" \
          "On Macos: configure Docker Engine in Docker Desktop settings." \
      - exit 1
    status:
      - (cat /etc/docker/daemon.json $HOME/.docker/daemon.json 2>/dev/null || true) | grep 'k3d-registry.virtualization-controller.test'

  _ensure:golangci-lint:
    desc: "Ensure golangci-lint is available"
    internal: true
    cmds:
      - |
        echo -e >&2 "Please install golangci-lint https://golangci-lint.run/usage/install/"
        exit 1
    status:
      - |
        [ -f ./golangci-lint ] || which golangci-lint

  _ensure:virtctl:
    desc: "Ensure virtctl tool is installed"
    internal: true
    cmds:
      - echo -e >&2 "Install virtctl from the release page of the KubeVirt github page:\n" \
          "https://github.com/kubevirt/kubevirt/releases\n"
      - exit 1
    status:
      - which virtctl >/dev/null
  _ensure:mirrord:
    desc: "Ensure mirrord tool is installed"
    internal: true
    cmds:
      - echo -e >&2 "Please install mirrord https://mirrord.dev/docs/overview/quick-start/#cli-tool"
      - exit 1
    status:
      - which mirrord >/dev/null


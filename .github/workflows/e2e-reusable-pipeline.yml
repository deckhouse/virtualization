# Copyright 2025 Flant JSC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: E2E Pipeline (Reusable)

on:
  workflow_call:
    inputs:
      storage_type:
        required: true
        type: string
        description: "Storage type (ceph or replicated or etc.)"
      nested_storageclass_name:
        required: true
        type: string
        description: "Nested storage class name"
      branch:
        required: false
        type: string
        default: "main"
        description: "Branch to use"
      virtualization_tag:
        required: false
        type: string
        default: "main"
        description: "Virtualization tag"
      deckhouse_tag:
        required: false
        type: string
        default: "main"
        description: "Deckhouse tag"
      pod_subnet_cidr:
        required: false
        type: string
        default: "10.88.0.0/16"
        description: "Pod subnet CIDR"
      service_subnet_cidr:
        required: false
        type: string
        default: "10.99.0.0/16"
        description: "Service subnet CIDR"
      default_user:
        required: false
        type: string
        default: "ubuntu"
        description: "Default user for vms"
      go_version:
        required: false
        type: string
        default: "1.24.6"
        description: "Go version"
      e2e_timeout:
        required: false
        type: string
        default: "3h"
        description: "E2E tests timeout"
    secrets:
      DEV_REGISTRY_DOCKER_CFG:
        required: true
      VIRT_E2E_NIGHTLY_SA_TOKEN:
        required: true
      PROD_IO_REGISTRY_DOCKER_CFG:
        required: true
      BOOTSTRAP_DEV_PROXY:
        required: true
    outputs:
      e2e-summary:
        description: "E2E test results"
        value: ${{ jobs.e2e-test.outputs.report-summary }}

env:
  BRANCH: ${{ inputs.branch }}
  VIRTUALIZATION_TAG: ${{ inputs.virtualization_tag }}
  DECKHOUSE_TAG: ${{ inputs.deckhouse_tag }}
  DEFAULT_USER: ${{ inputs.default_user }}
  GO_VERSION: ${{ inputs.go_version }}
  SETUP_CLUSTER_TYPE_PATH: test/dvp-static-cluster

defaults:
  run:
    shell: bash

jobs:
  bootstrap:
    name: Bootstrap cluster (${{ inputs.storage_type }})
    runs-on: ubuntu-latest
    concurrency:
      group: "${{ github.workflow }}-${{ github.event.number || github.ref }}-${{ inputs.storage_type }}"
      cancel-in-progress: true
    outputs:
      kubeconfig-content: ${{ steps.generate-kubeconfig.outputs.config }}
      storage-type: ${{ steps.vars.outputs.storage_type }}
      nested-storageclass-name: ${{ steps.vars.outputs.nested_storageclass_name }}
    steps:
      - uses: actions/checkout@v4
        # with:
        #   ref: ${{ env.BRANCH }}

      - name: Set outputs
        id: vars
        run: |
          namespace="nightly-e2e-${{ inputs.storage_type }}-$(git rev-parse --short HEAD)"
          echo "namespace=$namespace" >> $GITHUB_OUTPUT
          echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
          echo "storage_type=${{ inputs.storage_type }}" >> $GITHUB_OUTPUT
          echo "nested_storageclass_name=${{ inputs.nested_storageclass_name }}" >> $GITHUB_OUTPUT

          REGISTRY=$(base64 -d <<< ${{secrets.DEV_REGISTRY_DOCKER_CFG}} | jq '.auths | to_entries | .[] | .key' -r)
          USERNAME=$(base64 -d <<< ${{ secrets.DEV_REGISTRY_DOCKER_CFG }} | jq '.auths | to_entries | .[] | .value.auth' -r | base64 -d | cut -d ':' -f1)
          PASSWORD=$(base64 -d <<< ${{ secrets.DEV_REGISTRY_DOCKER_CFG }} | jq '.auths | to_entries | .[] | .value.auth' -r | base64 -d | cut -d ':' -f2)

          echo "registry=$REGISTRY" >> $GITHUB_OUTPUT
          echo "username=$USERNAME" >> $GITHUB_OUTPUT
          echo "password=$PASSWORD" >> $GITHUB_OUTPUT

      - name: Install htpasswd utility
        run: |
          sudo apt-get update
          sudo apt-get install -y apache2-utils

      - name: Install Task
        uses: arduino/setup-task@v2
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup d8
        uses: ./.github/actions/install-d8

      - name: Log in to private registry
        uses: docker/login-action@v3
        with:
          registry: ${{ steps.vars.outputs.registry }}
          username: ${{ steps.vars.outputs.username }}
          password: ${{ steps.vars.outputs.password }}

      - name: Configure kubectl via azure/k8s-set-context@v4
        uses: azure/k8s-set-context@v4
        with:
          method: kubeconfig
          context: e2e-cluster-nightly-e2e-virt-sa
          kubeconfig: ${{ secrets.VIRT_E2E_NIGHTLY_SA_TOKEN }}

      - name: Generate values.yaml
        working-directory: ${{ env.SETUP_CLUSTER_TYPE_PATH }}
        run: |
          defaultStorageClass=$(kubectl get storageclass -o json \
            | jq -r '.items[] | select(.metadata.annotations."storageclass.kubernetes.io/is-default-class" == "true") | .metadata.name')

          cat <<EOF > values.yaml
          namespace: ${{ steps.vars.outputs.namespace }}
          storage_type: ${{ inputs.storage_type }}
          storageClass: ${defaultStorageClass}
          sa: dkp-sa
          deckhouse:
            podSubnetCIDR: ${{ inputs.pod_subnet_cidr }}
            serviceSubnetCIDR: ${{ inputs.service_subnet_cidr }}
            tag: ${{ env.DECKHOUSE_TAG }}
            kubernetesVersion: Automatic
            registryDockerCfg: ${{ secrets.DEV_REGISTRY_DOCKER_CFG }}
            bundle: Default
            proxy:
              httpProxy: ${{ secrets.BOOTSTRAP_DEV_PROXY }}
              httpsProxy: ${{ secrets.BOOTSTRAP_DEV_PROXY }}
              noProxy: 
              - "localhost"
              - "127.0.0.1"
              - "10.0.0.0/8"
              - "172.16.0.0/12"
              - "192.168.0.0/16"
              - "10.112.0.0/16"
              - "10.223.0.0/16"
              - "docker.io"
              - ".ubuntu.com"
          image:
            url: https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img
            defaultUser: ${{ env.DEFAULT_USER }}
            bootloader: BIOS
          ingressHosts:
            - api
            - grafana
            - dex
            - prometheus
            - console
            - virtualization
          instances:
            masterNodes:
              count: 1
              cfg:
                rootDiskSize: 60Gi
                cpu:
                  cores: 4
                  coreFraction: 50%
                memory:
                  size: 12Gi
            additionalNodes:
            - name: worker
              count: 3
              cfg:
                cpu:
                  cores: 4
                  coreFraction: 50%
                memory:
                  size: 6Gi
                additionalDisks:
                - size: 50Gi
          EOF

      - name: Bootstrap cluster [infra-deploy]
        working-directory: ${{ env.SETUP_CLUSTER_TYPE_PATH }}
        run: |
          task infra-deploy
      - name: Bootstrap cluster [dhctl-bootstrap]
        id: dhctl-bootstrap
        working-directory: ${{ env.SETUP_CLUSTER_TYPE_PATH }}
        run: |
          task dhctl-bootstrap
        timeout-minutes: 30
      - name: Bootstrap cluster [show-connection-info]
        working-directory: ${{ env.SETUP_CLUSTER_TYPE_PATH }}
        run: |
          task show-connection-info

      - name: Save ssh to secrets in cluster
        env:
          NAMESPACE: ${{ steps.vars.outputs.namespace }}
        if: always()
        run: |
          kubectl -n $NAMESPACE create secret generic ssh-key --from-file=${{ env.SETUP_CLUSTER_TYPE_PATH }}/tmp/ssh/cloud

      - name: Get info about nested master VM
        working-directory: ${{ env.SETUP_CLUSTER_TYPE_PATH }}
        env:
          NAMESPACE: ${{ steps.vars.outputs.namespace }}
          PREFIX: ${{ inputs.storage_type }}
        run: |
          nested_master=$(kubectl -n ${NAMESPACE} get vm -l group=${PREFIX}-master -o jsonpath="{.items[0].metadata.name}")

          echo "Pods"
          kubectl get pods -n "${NAMESPACE}"
          echo ""

          echo "VMs"
          kubectl get vm -n "${NAMESPACE}"
          echo ""

          echo "VDs"
          kubectl get vd -n "${NAMESPACE}"
          echo ""

          echo "login to master"
          echo "os-release master"
          d8 v ssh -i ./tmp/ssh/cloud \
            --local-ssh=true \
            --local-ssh-opts="-o StrictHostKeyChecking=no" \
            --local-ssh-opts="-o UserKnownHostsFile=/dev/null" \
            ${DEFAULT_USER}@${nested_master}.${NAMESPACE} \
            -c 'cat /etc/os-release'
          echo ""

          echo "hostname master"
          d8 v ssh -i ./tmp/ssh/cloud  \
            --local-ssh=true \
            --local-ssh-opts="-o StrictHostKeyChecking=no" \
            --local-ssh-opts="-o UserKnownHostsFile=/dev/null" \
            ${DEFAULT_USER}@${nested_master}.${NAMESPACE} \
            -c 'hostname'

      - name: Generate nested kubeconfig
        id: generate-kubeconfig
        working-directory: ${{ env.SETUP_CLUSTER_TYPE_PATH }}
        env:
          kubeConfigPath: tmp/kube.config
          NAMESPACE: ${{ steps.vars.outputs.namespace }}
          PREFIX: ${{ inputs.storage_type }}
        run: |
          nested_master=$(kubectl -n ${NAMESPACE} get vm -l group=${PREFIX}-master -o jsonpath="{.items[0].metadata.name}")

          d8vscp() {
            local source=$1
            local dest=$2
            d8 v scp -i ./tmp/ssh/cloud \
              --local-ssh=true \
              --local-ssh-opts="-o StrictHostKeyChecking=no" \
              --local-ssh-opts="-o UserKnownHostsFile=/dev/null" \
              $source $dest
            echo "d8vscp: $source -> $dest - done"
          }

          d8vssh() {
            local cmd=$1
            d8 v ssh -i ./tmp/ssh/cloud \
            --local-ssh=true \
            --local-ssh-opts="-o StrictHostKeyChecking=no" \
            --local-ssh-opts="-o UserKnownHostsFile=/dev/null" \
            ${DEFAULT_USER}@${nested_master}.${NAMESPACE} \
            -c "$cmd"
          }

          echo "[INFO] Copy script for generating kubeconfig in nested cluster"
          echo "[INFO] Copy tools/gen-kubeconfig.sh to master"
          d8vscp "./tools/gen-kubeconfig.sh" "${DEFAULT_USER}@${nested_master}.${NAMESPACE}:/tmp/gen-kubeconfig.sh"
          echo ""
          d8vscp "./tools/deckhouse-queue.sh" "${DEFAULT_USER}@${nested_master}.${NAMESPACE}:/tmp/deckhouse-queue.sh"
          echo ""

          echo "[INFO] Set file exec permissions"
          d8vssh 'chmod +x /tmp/{gen-kubeconfig.sh,deckhouse-queue.sh}'
          d8vssh 'ls -la /tmp/'
          echo "[INFO] Check d8 queue in nested cluster"
          d8vssh 'sudo /tmp/deckhouse-queue.sh'

          echo "[INFO] Generate kube conf in nested cluster"
          echo "[INFO] Run gen-kubeconfig.sh in nested cluster"
          d8vssh "sudo /tmp/gen-kubeconfig.sh nested-sa nested nested-e2e /${kubeConfigPath}"
          echo ""

          echo "[INFO] Copy kubeconfig to runner"
          echo "[INFO] ${DEFAULT_USER}@${nested_master}.$NAMESPACE:/${kubeConfigPath} ./${kubeConfigPath}"
          d8vscp "${DEFAULT_USER}@${nested_master}.$NAMESPACE:/${kubeConfigPath}" "./${kubeConfigPath}"

          echo "[INFO] Set rights for kubeconfig"
          echo "[INFO] sudo chown 1001:1001 ${kubeConfigPath}"
          sudo chown 1001:1001 ${kubeConfigPath}
          echo " "

          echo "[INFO] Kubeconf to github output"
          CONFIG=$(cat ${kubeConfigPath} | base64 -w 0)
          CONFIG=$(echo $CONFIG | base64 -w 0)
          echo "config=$CONFIG" >> $GITHUB_OUTPUT

      - name: cloud-init logs
        if: steps.dhctl-bootstrap.outcome == 'failure'
        env:
          NAMESPACE: ${{ steps.vars.outputs.namespace }}
          PREFIX: ${{ inputs.storage_type }}
        run: |
          nested_master=$(kubectl -n ${NAMESPACE} get vm -l group=${PREFIX}-master -o jsonpath="{.items[0].metadata.name}")

          d8vscp() {
            local source=$1
            local dest=$2
            d8 v scp -i ./tmp/ssh/cloud \
              --local-ssh=true \
              --local-ssh-opts="-o StrictHostKeyChecking=no" \
              --local-ssh-opts="-o UserKnownHostsFile=/dev/null" \
              $source $dest
            echo "d8vscp: $source -> $dest - done"
          }

          d8vscp "${DEFAULT_USER}@${nested_master}.$NAMESPACE:/var/log/cloud-init*.log" "./${{ env.SETUP_CLUSTER_TYPE_PATH }}/tmp/"

      - name: Prepare artifact
        if: always()
        run: |
          sudo chown -fR 1001:1001 ${{ env.SETUP_CLUSTER_TYPE_PATH }}
          yq e '.deckhouse.registryDockerCfg = "None"' -i ./${{ env.SETUP_CLUSTER_TYPE_PATH }}/values.yaml
          yq e 'select(.kind == "InitConfiguration") .deckhouse.registryDockerCfg = "None"' -i ./${{ env.SETUP_CLUSTER_TYPE_PATH }}/tmp/config.yaml || echo "The config.yaml file is not generated, skipping"
          echo "${{ steps.generate-kubeconfig.outputs.config }}" | base64 -d | base64 -d > ./${{ env.SETUP_CLUSTER_TYPE_PATH }}/kube-config

      - name: Upload generated files
        uses: actions/upload-artifact@v4
        id: artifact-upload
        if: always()
        with:
          name: generated-files-${{ inputs.storage_type }}
          path: |
            ${{ env.SETUP_CLUSTER_TYPE_PATH }}/tmp
            ${{ env.SETUP_CLUSTER_TYPE_PATH }}/values.yaml
          overwrite: true
          include-hidden-files: true
          retention-days: 1

      - name: Upload ssh config
        uses: actions/upload-artifact@v4
        id: artifact-upload-ssh
        if: always()
        with:
          name: generated-files-ssh-${{ inputs.storage_type }}
          path: ${{ env.SETUP_CLUSTER_TYPE_PATH }}/tmp/ssh
          overwrite: true
          include-hidden-files: true
          retention-days: 1

      - name: Upload kubeconfig config
        uses: actions/upload-artifact@v4
        id: artifact-upload-kubeconfig
        if: always()
        with:
          name: generated-files-kubeconfig-${{ inputs.storage_type }}
          path: ${{ env.SETUP_CLUSTER_TYPE_PATH }}/kube-config
          overwrite: true
          include-hidden-files: true
          retention-days: 1

  configure-storage:
    name: Configure storage (${{ inputs.storage_type }})
    runs-on: ubuntu-latest
    needs: bootstrap
    steps:
      - uses: actions/checkout@v4

      - name: Install Task
        uses: arduino/setup-task@v2
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup d8
        uses: ./.github/actions/install-d8
      - name: Install kubectl CLI
        uses: azure/setup-kubectl@v4

      - name: Check kubeconfig
        run: |
          mkdir -p ~/.kube
          echo "[INFO] Configure kubeconfig for nested cluster"
          echo "${{ needs.bootstrap.outputs.kubeconfig-content }}" | base64 -d | base64 -d > ~/.kube/config

          echo "[INFO] Show paths and files content"
          ls -la ~/.kube
          echo "[INFO] Set permissions for kubeconfig"
          chmod 600 ~/.kube/config

          echo "[INFO] Show nodes in cluster"
          kubectl config use-context nested-e2e-nested-sa

          # some times kubectl get nodes returns error, so we need to retry
          for i in {1..3}; do
            echo "Attempt $i/3..."
            if (kubectl get nodes); then
              echo "[SUCCESS] Successfully retrieved nodes."
              break
            else
              echo "[INFO] Retrying in 5 seconds..."
              sleep 5
            fi
          done

      - name: Configure replicated storage
        if: ${{ inputs.storage_type == 'replicated' }}
        working-directory: ${{ env.SETUP_CLUSTER_TYPE_PATH }}/storage/sds-replicated
        run: |
          kubectl apply -f mc.yaml
          echo "[INFO] Wait for sds-node-configurator"
          kubectl wait --for=jsonpath='{.status.phase}'=Ready modules sds-node-configurator --timeout=300s

          for i in {1..60}; do
            sds_replicated_volume_status=$(kubectl get ns d8-sds-replicated-volume -o jsonpath='{.status.phase}' || echo "False")
            
            if [[ "${sds_replicated_volume_status}" = "Active" ]]; then
              echo "[SUCCESS] Namespaces sds-replicated-volume are Active"
              kubectl -n d8-sds-replicated-volume get pods
              break
            fi
            
            echo "[INFO] Waiting 10s for sds-replicated-volume to be ready"
            echo "[INFO] Show namespaces sds-replicated-volume"
            kubectl get ns | grep sds-replicated-volume || echo "Namespaces sds-replicated-volume are not ready"
            
            if (( i % 5 == 0 )); then
              d8 s queue list | head -n25 || echo "No queues"
            fi
            sleep 10
          done

          echo "[INFO] Wait BlockDevice are ready"
          workers=$(kubectl get nodes -o name | grep -c worker)
          workers=$((workers))
          bdexists=false
          count=60
          for i in $(seq 1 $count); do
            blockdevices=$(kubectl get blockdevice -o name | wc -l)
            if [ $blockdevices -ge $workers ]; then
              bdexists=true
              break
            fi
            echo "[INFO] Wait 10 sec until blockdevices is greater or equal to $workers [${i}/${count}]"
            if (( i % 5 == 0 )); then
              d8 s queue list | head -n25 || echo "No queues"
            fi
            sleep 10
          done

          if [ $bdexists = false ]; then
            echo "[ERROR] Blockdevices is not 3"
            echo "[DEBUG] Show blockdevice"
            kubectl get blockdevice
            echo "[DEBUG] Show sds namespaces"
            kubectl get ns | grep sds || echo "ns sds is not found"
            echo "[DEBUG] Show cluster nodes"
            kubectl get nodes
            echo "[DEBUG] Show deckhouse logs"
            d8 s logs | tail -n 100
            echo " "
            exit 1
          fi

          echo "[INFO] Wait pods and webhooks sds-replicated pods"
          for i in {1..60}; do
            echo "[INFO] Check sds-replicated pods, linstor-node csi-node webhooks"
            linstor_node=$(kubectl -n d8-sds-replicated-volume get po 2>/dev/null | grep linstor-node | grep -c Running || echo 0)
            csi_node=$(kubectl -n d8-sds-replicated-volume get po 2>/dev/null | grep csi-node | grep -c Running || echo 0)
            webhooks=$(kubectl -n d8-sds-replicated-volume get po 2>/dev/null | grep webhooks | grep -c Running || echo 0)

            linstor_node=$((linstor_node))
            csi_node=$((csi_node))
            webhooks=$((webhooks))
            
            echo "[INFO] Check if sds-replicated pods are ready"
            if [[ "${linstor_node}" -ge ${workers} && "${csi_node}" -ge ${workers} && ${webhooks} -ge 1 ]]; then
              echo "[SUCCESS] sds-replicated-volume is ready"
              break
            fi

            echo "[INFO] Waiting 10s for sds-replicated-volume to be ready"
            if (( i % 5 == 0 )); then
              echo "[DEBUG] Get pods"
              kubectl -n d8-sds-replicated-volume get pods || true
              echo "[DEBUG] Show queue (first 25 lines)"
              d8 s queue list | head -n 25 || echo "Failed to retrieve list queue"
              echo " "
            fi
          done

          chmod +x lvg-gen.sh
          ./lvg-gen.sh

          chmod +x rsc-gen.sh
          ./rsc-gen.sh

          echo "[INFO] Enshure that nested storageclasses are created"
          kubectl get storageclass | grep nested || echo "[WARNING] No nested storageclasses"
          echo "[SUCCESS] Done"
      - name: Configure ceph storage
        if: ${{ inputs.storage_type == 'ceph' }}
        working-directory: ${{ env.SETUP_CLUSTER_TYPE_PATH }}/storage/ceph
        run: |
          d8_queue_list() {
            d8 s queue list | grep -Po '([0-9]+)(?= active)' || echo "[WARNING] Failed to retrieve list queue"
          }

          d8_queue() {
            local count=90

            for i in $(seq 1 $count) ; do
              if [[ "$(d8_queue_list)" == "0" ]]; then
                echo "[SUCCESS] Queue is clear"
                break
              else
                echo "[INFO] Show queue list"
                d8 s queue list | head -n25 || echo "[WARNING] Failed to retrieve list queue"
              fi

              echo "[INFO] Wait until queues are empty ${i}/${count}"
              kubectl get ns | grep sds || echo "Namespaces sds not found"
              echo "   "
              sleep 10
            done
          }

          export registry=${{ secrets.PROD_IO_REGISTRY_DOCKER_CFG }}
          yq e '.spec.registry.dockerCfg = env(registry)' -i 00-ms.yaml
          unset registry

          echo "[INFO] Create prod module source"
          kubectl apply -f 00-ms.yaml
          kubectl wait --for=jsonpath='{.status.phase}' modulesource deckhouse-prod --timeout=30s
          kubectl get modulesources

          echo "[INFO] Create ceph operator and csi module config"
          kubectl apply -f 01-mc.yaml

          echo "[INFO] Wait while queues are empty"
          d8_queue

          echo "Start wait for ceph operator and csi"
          for i in {1..60}; do
            ceph_operator_status=$(kubectl get ns d8-operator-ceph -o jsonpath='{.status.phase}' || echo "False")
            csi_ceph_status=$(kubectl get module csi-ceph -o jsonpath='{.status.phase}' || echo "False")
            
            if [[ "${ceph_operator_status}" = "Active" && "${csi_ceph_status}" = "Ready" ]]; then
              echo "[SUCCESS] Namespaces operator-ceph and csi are Active"
              break
            fi
            
            echo "[INFO] Waiting 10s for ceph operator and csi namespaces to be ready"
            echo "[INFO] Get namespace"
            kubectl get namespace | grep ceph || echo "[WARNING] Namespaces operator-ceph and csi are not ready"
            
            if (( i % 5 == 0 )); then
              echo "[DEBUG] Show all namespaces"
              kubectl get namespace
              echo " "
              d8 s queue list | head -n25 || echo "[WARNING] Failed to retrieve list queue"
            fi
            sleep 10
          done

          echo "[INFO] Create ServiceAccounts"
          kubectl apply -f 02-sa.yaml
          echo "[INFO] Create ConfigMap (patch existing for configure rbd support)"
          kubectl apply -f 03-cm.yaml
          echo "[INFO] Create Cluster"
          kubectl apply -f 04-cluster.yaml

          echo "[INFO] Get pod in d8-operator-ceph"
          kubectl -n d8-operator-ceph get po

          echo "[INFO] Wait for ceph operator"
          for i in {1..60}; do
            echo "[INFO] Check ceph pods, mon mgr osd"
            ceph_mgr=$(kubectl -n d8-operator-ceph get po 2>/dev/null | grep ceph-mgr | grep -c Running)
            ceph_mon=$(kubectl -n d8-operator-ceph get po 2>/dev/null | grep ceph-mon | grep -c Running)
            ceph_osd=$(kubectl -n d8-operator-ceph get po 2>/dev/null | grep ceph-osd | grep -c Running)

            ceph_mgr=$((ceph_mgr))
            ceph_mon=$((ceph_mon))
            ceph_osd=$((ceph_osd))
            
            echo "[INFO] check if ceph pods are ready"
            if [[ $ceph_mgr -ge 2 && $ceph_mon -ge 3 && $ceph_osd -ge 3 ]]; then
              echo "[SUCCESS] Ceph cluster is ready"
              break
            fi

            echo "[WARNING] Not all pods are ready, ceph_mgr=${ceph_mgr}, ceph_mon=${ceph_mon}, ceph_osd=${ceph_osd}"
            echo "[INFO] Waiting 10s for ceph operator to be ready"
            kubectl -n d8-operator-ceph get po || echo "Failed to retrieve pods"
            if (( i % 5 == 0 )); then
              echo "[DEBUG] Show ceph namespace"
              kubectl get ns | grep ceph || echo "Failed to retrieve ceph ns"
              echo "[DEBUG] Show ModuleConfig ceph"
              kubectl get mc | grep ceph || echo "Failed to retrieve mc"
              echo "[DEBUG] Show ceph in resource modules"
              kubectl get modules -o wide | grep ceph || echo "Failed to retrieve modules"
              echo "[DEBUG] Show queue"
              d8 s queue list | head -n 25 || echo "Failed to retrieve list queue"
            fi
            echo "[INFO] Wait until all necessary pods are ready ${i}/60"
            sleep 10
          done

          echo "[INFO] Show pods"
          kubectl get pods -n d8-operator-ceph

          kubectl apply -f 05-blockpool.yaml
          echo "[INFO] Wait for ceph-rbd-pool-r2 blockpool to be ready, timeout 600s"
          kubectl -n d8-operator-ceph wait --for=jsonpath='{.status.phase}'=Ready cephblockpools.ceph.rook.io ceph-rbd-pool-r2 --timeout=600s
          kubectl apply -f 06-toolbox.yaml
          echo "[INFO] Wait for rook-ceph-tools, timeout 300s"
          kubectl -n d8-operator-ceph wait --for=condition=Available deployment/rook-ceph-tools --timeout=300s

          echo "[INFO] Show ceph pools via rook-ceph-tools"
          kubectl -n d8-operator-ceph exec deployments/rook-ceph-tools -c ceph-tools -- ceph osd pool ls

          echo "[INFO] Configure storage class"
          chmod +x ./ceph-configure.sh
          ./ceph-configure.sh

  configure-virtualization:
    name: Configure Virtualization (${{ inputs.storage_type }})
    runs-on: ubuntu-22.04
    needs:
      - bootstrap
      - configure-storage
    steps:
      - uses: actions/checkout@v4
      - name: Install kubectl CLI
        uses: azure/setup-kubectl@v4
      - name: Setup d8
        uses: ./.github/actions/install-d8

      - name: Check kubeconfig
        run: |
          echo "[INFO] Configure kube config"
          mkdir -p ~/.kube
          echo "${{ needs.bootstrap.outputs.kubeconfig-content }}" | base64 -d | base64 -d > ~/.kube/config
          chmod 600 ~/.kube/config
          kubectl config use-context nested-e2e-nested-sa

      - name: Configure Virtualization
        run: |
          echo "[INFO] Apply Virtualization module config"
          kubectl apply -f -<<EOF
          apiVersion: deckhouse.io/v1alpha1
          kind: ModuleConfig
          metadata:
            name: virtualization
          spec:
            enabled: true
            settings:
              # https:
              #   certManager:
              #     clusterIssuerName: selfsigned
              #   mode: CertManager
              dvcr:
                storage:
                  persistentVolumeClaim:
                    size: 10Gi
                    storageClassName: ${{ inputs.nested_storageclass_name }}
                  type: PersistentVolumeClaim
              virtualMachineCIDRs:
                - 192.168.10.0/24
            source: deckhouse
            version: 1
          ---
          apiVersion: deckhouse.io/v1alpha2
          kind: ModulePullOverride
          metadata:
            name: virtualization
          spec:
            imageTag: ${{ env.VIRTUALIZATION_TAG }}
            scanInterval: 10m
          EOF

          echo "[INFO] Show module config virtualization info"
          kubectl get mc virtualization

          echo "[INFO] Show ModulePullOverride virtualization info"
          kubectl get mpo virtualization
      - name: Wait for Virtualization to be ready
        run: |
          d8_queue_list() {
            d8 s queue list | grep -Po '([0-9]+)(?= active)' || echo "Failed to retrieve list queue"
          }

          d8_queue() {
            local count=90

            for i in $(seq 1 $count) ; do
              if [ $(d8_queue_list) == "0" ]; then
                echo "[SUCCESS] Queues are empty"
                break
              else
                echo "[INFO] Show queue list"
                d8 s queue list | head -n25 || echo "Failed to retrieve list queue"
              fi

              echo "[INFO] Wait until queues are empty ${i}/${count}"
              
              if (( i % 5 == 0 )); then
                echo " "
                d8 s logs | tail -n 100
                echo " "
              fi
              sleep 10
            done
          }
          
          virttualization_ready() {
            local count=90
            local virtualization_status

            for i in $(seq 1 $count) ; do
              virtualization_status=$(kubectl get modules virtualization -o jsonpath='{.status.phase}')
              if [ "$virtualization_status" == "Ready" ]; then
                echo "[SUCCESS] Virtualization module is ready"
                kubectl get modules virtualization
                kubectl -n d8-virtualization get pods
                break
              fi
              
              echo "[INFO] Waiting 10s for Virtualization module to be ready (attempt $i/$count)"
              
              if (( i % 5 == 0 )); then
                echo " "
                echo "[DEBUG] Show additional info"
                kubectl get ns d8-virtualization || echo "[WARNING] Namespace virtualization is not ready"
                echo " "
                kubectl -n d8-virtualization get pods || echo "[WARNING] Pods in namespace virtualization is not ready"
                kubectl get pvc -n d8-virtualization || echo "[WARNING] PVC in namespace virtualization is not ready"
                echo " "
              fi
              sleep 10
            done
            
            echo "[ERROR] Virtualization module deploy failed"
            echo "[DEBUG] Show describe virtualization module"
            echo "::group::ðŸ“¦ describe virtualization module"
            kubectl describe modules virtualization || true
            echo "::endgroup::"
            echo "[DEBUG] Show namespace d8-virtualization"
            kubectl get ns d8-virtualization || true
            echo "[DEBUG] Show pods in namespace d8-virtualization"
            kubectl -n d8-virtualization get pods || true
            echo "[DEBUG] Show dvcr pod yaml"
            echo "::group::ðŸ“¦ dvcr pod yaml"
            kubectl -n d8-virtualization get pod -l app=dvcr -o yaml || true
            echo "::endgroup::"
            echo "[DEBUG] Show dvcr pod describe"
            echo "::group::ðŸ“¦ dvcr pod describe"
            kubectl -n d8-virtualization describe pod -l app=dvcr || true
            echo "::endgroup::"
            echo "[DEBUG] Show pvc in namespace d8-virtualization"
            kubectl get pvc -n d8-virtualization || true
            echo "[DEBUG] Show storageclasses"
            kubectl get storageclasses || true
            echo "[DEBUG] Show queue (first 25 lines)"
            d8 s queue list | head -n 25 || echo "[WARNING] Failed to retrieve list queue"
            echo "[DEBUG] Show deckhouse logs"
            echo "::group::ðŸ“ deckhouse logs"
            d8 s logs | tail -n 100
            echo "::endgroup::"
            exit 1
          }

          echo " "
          echo "[INFO] Waiting for Virtualization module to be ready"
          d8_queue
          
          virttualization_ready
  e2e-test:
    name: E2E test (${{ inputs.storage_type }})
    runs-on: ubuntu-22.04
    outputs:
      report-summary: ${{ steps.e2e-summary.outputs.summary }}
    needs:
      - bootstrap
      - configure-storage
      - configure-virtualization
    steps:
      - uses: actions/checkout@v4

      - name: Set up Go ${{ env.GO_VERSION }}
        uses: actions/setup-go@v5
        with:
          go-version: "${{ env.GO_VERSION }}"

      - name: Install Task
        uses: arduino/setup-task@v2
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Install ginkgo
        working-directory: ./test/e2e/
        run: |
          echo "Install ginkgo"
          go install tool

      - name: Setup d8
        uses: ./.github/actions/install-d8

      - name: Install kubectl CLI
        uses: azure/setup-kubectl@v4

      - name: Check kubeconfig
        run: |
          echo "Configure kube config"
          mkdir -p ~/.kube
          echo "${{ needs.bootstrap.outputs.kubeconfig-content }}" | base64 -d | base64 -d > ~/.kube/config
          chmod 600 ~/.kube/config
          kubectl config use-context nested-e2e-nested-sa
          kubectl get vmclass

      - name: Download dependencies
        working-directory: ./test/e2e/
        run: |
          echo "Download dependencies"
          go mod download

      - name: Create vmclass for e2e tests
        run: |
          kubectl get vmclass/generic -o json | jq 'del(.status) | del(.metadata) | .metadata = {"name":"generic-for-e2e","annotations":{"virtualmachineclass.virtualization.deckhouse.io/is-default-class":"true"}} ' | kubectl create -f -

      - name: Run E2E
        id: e2e-summary
        env:
          TIMEOUT: ${{ inputs.e2e_timeout }}
          CSI: ${{ inputs.storage_type }}
        working-directory: ./test/e2e/
        run: |
          if [[ "${{ inputs.storage_type }}" == "replicated" ]]; then
            export SKIP_IMMEDIATE_SC_CHECK="yes"
          fi
          STORAGE_CLASS_NAME=${{ inputs.nested_storageclass_name }} FOCUS="VirtualMachineConfiguration" task run:ci -v LABELS="Slow"

          echo "summary=$SUMMARY" >> $GITHUB_OUTPUT
          summary_file_name="e2e_summary_${{ inputs.storage_type }}_$DATE.json"
          echo "report_file_name=${summary_file_name}" >> $GITHUB_OUTPUT
          
          echo $SUMMARY > "${summary_file_name}"

      - name: Upload summary test results
        uses: actions/upload-artifact@v4
        id: e2e-summary-artifact
        if: always()
        with:
          name: ${{ steps.e2e-summary.outputs.report_file_name }}
          path: test/e2e/e2e_summary_${{ inputs.storage_type }}.json
          if-no-files-found: ignore

  undeploy-cluster:
    name: Undeploy cluster (${{ inputs.storage_type }})
    runs-on: ubuntu-latest
    needs:
      - bootstrap
      - configure-storage
      - configure-virtualization
      - e2e-test
    # if: always()
    if: cancelled() || success()
    steps:
      - uses: actions/checkout@v4

      - name: Install htpasswd utility
        run: |
          sudo apt-get update
          sudo apt-get install -y apache2-utils

      - name: Setup d8
        uses: ./.github/actions/install-d8

      - name: Install Task
        uses: arduino/setup-task@v2
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download artifacts
        uses: actions/download-artifact@v5
        with:
          name: generated-files-${{ inputs.storage_type }}
          path: ${{ env.SETUP_CLUSTER_TYPE_PATH }}/

      - name: Configure kubectl via azure/k8s-set-context@v4
        uses: azure/k8s-set-context@v4
        with:
          method: kubeconfig
          context: e2e-cluster-nightly-e2e-virt-sa
          kubeconfig: ${{ secrets.VIRT_E2E_NIGHTLY_SA_TOKEN }}

      - name: infra-undeploy
        working-directory: ${{ env.SETUP_CLUSTER_TYPE_PATH }}
        run: |
          task infra-undeploy

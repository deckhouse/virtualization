# Copyright 2025 Flant JSC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: E2E Matrix Tests (DVP-over-DVP)

on:
  push:
    branches:
      - feat/ci-e2e-matrix
  pull_request:
    types: [opened, reopened, synchronize, labeled, unlabeled]
    branches:
      - main
      - feat/ci-e2e-matrix
  schedule:
    - cron: "30 2 * * *"
  workflow_dispatch:
    inputs:
      profiles:
        description: "Storage profiles (comma-separated): sds, cephrbd"
        required: false
        default: "sds,cephrbd"
      timeout:
        description: "Ginkgo timeout (e.g. 2h, 4h)"
        required: false
        default: "4h"

permissions:
  contents: read

env:
  E2E_K8S_URL: https://api.e2e.virtlab.flant.com

jobs:
  # ============================================
  # 1. SETUP - Environment preparation
  # ============================================
  setup:
    name: Setup Environment
    runs-on: ubuntu-latest
    outputs:
      profiles: ${{ steps.load.outputs.profiles }}
    steps:
      - uses: actions/checkout@v4

      - name: Load storage profiles
        id: load
        run: |
          # Load profiles dynamically from profiles.json
          cd ci/dvp-e2e
          PROFILES=$(jq -r '[.[].name] | @json' profiles.json)
          echo "profiles=$PROFILES" >> "$GITHUB_OUTPUT"

      - name: Print matrix
        run: |
          echo "Will test profiles: ${{ steps.load.outputs.profiles }}"

  # ============================================
  # 2. E2E - Parallel test execution
  # ============================================
  prepare:
    name: Prepare Cluster (${{ matrix.profile }})
    needs: [setup]
    runs-on: ubuntu-latest
    timeout-minutes: 300
    concurrency:
      group: prepare-${{ github.ref }}-${{ matrix.profile }}
      cancel-in-progress: true
    strategy:
      fail-fast: false
      matrix:
        profile: ${{ fromJson(needs.setup.outputs.profiles) }}

    env:
      GO_VERSION: "1.24.6"
      TMP_ROOT: ${{ github.workspace }}/ci/dvp-e2e/tmp
      LOOP_WEBHOOK: ${{ secrets.LOOP_WEBHOOK_URL || secrets.LOOP_WEBHOOK }}
      LOOP_CHANNEL: ${{ secrets.LOOP_CHANNEL || 'test-virtualization-loop-alerts' }}  # TODO: replace with channel secret after successful run

    outputs:
      run_id: ${{ steps.prep.outputs.run_id }}
      storage_class: ${{ steps.profile-config.outputs.storage_class }}
      image_storage_class: ${{ steps.profile-config.outputs.image_storage_class }}
      snapshot_storage_class: ${{ steps.profile-config.outputs.snapshot_storage_class }}
      attach_disk_size: ${{ steps.profile-config.outputs.attach_disk_size }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: Install Task
        uses: arduino/setup-task@v2
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Install Helm
        uses: azure/setup-helm@v4.3.0
        with:
          version: v3.17.2

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: Install Deckhouse CLI
        env:
          D8_VERSION: v0.13.2
        run: |
          set -euo pipefail
          echo "Installing d8 ${D8_VERSION}..."
          curl -fsSL -o d8.tgz "https://deckhouse.io/downloads/deckhouse-cli/${D8_VERSION}/d8-${D8_VERSION}-linux-amd64.tar.gz"
          tar -xzf d8.tgz linux-amd64/bin/d8
          mv linux-amd64/bin/d8 /usr/local/bin/d8
          chmod +x /usr/local/bin/d8
          rm -rf d8.tgz linux-amd64
          d8 --version

      - name: Install yq
        run: |
          echo "Installing yq..."
          curl -L -o /usr/local/bin/yq https://github.com/mikefarah/yq/releases/download/v4.44.1/yq_linux_amd64
          chmod +x /usr/local/bin/yq

      - name: Prepare environment
        id: prep
        run: |
          RUN_ID="nightly-nested-e2e-${{ matrix.profile }}-$(date +%H%M)"
          echo "run_id=$RUN_ID" >> "$GITHUB_OUTPUT"
          echo "RUN_ID=$RUN_ID" >> "$GITHUB_ENV"
          echo "PROFILE=${{ matrix.profile }}" >> "$GITHUB_ENV"
          echo "TMP_ROOT=${{ env.TMP_ROOT }}" >> "$GITHUB_ENV"
          mkdir -p "${{ env.TMP_ROOT }}/shared" "${{ env.TMP_ROOT }}/matrix-logs"

      - name: Build parent kubeconfig from secret
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$HOME/.kube"
          cat > "$HOME/.kube/config" <<EOF
          apiVersion: v1
          kind: Config
          clusters:
          - cluster:
              server: ${E2E_K8S_URL}
              insecure-skip-tls-verify: true
            name: parent
          contexts:
          - context:
              cluster: parent
              user: sa
            name: parent
          current-context: parent
          users:
          - name: sa
            user:
              token: "${{ secrets.E2E_NESTED_SA_SECRET }}"
EOF
          chmod 600 "$HOME/.kube/config"
          echo "KUBECONFIG=$HOME/.kube/config" >> "$GITHUB_ENV"

      - name: Prepare run values.yaml
        working-directory: ci/dvp-e2e
        run: |
          task run:values:prepare \
            RUN_ID="${{ env.RUN_ID }}" \
            RUN_NAMESPACE="${{ env.RUN_ID }}" \
            RUN_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}"
          echo "VALUES_TEMPLATE_FILE=${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" >> $GITHUB_ENV

      - name: Prepare Deckhouse registry auth
        run: |
          set -euo pipefail
          prod_user='${{ secrets.PROD_READ_REGISTRY_USER }}'
          prod_pass='${{ secrets.PROD_READ_REGISTRY_PASSWORD }}'
          dev_user='${{ secrets.BOOTSTRAP_DEV_REGISTRY_LOGIN }}'
          dev_pass='${{ secrets.BOOTSTRAP_DEV_REGISTRY_PASSWORD }}'
          if [ -z "$prod_user" ] || [ -z "$prod_pass" ]; then
            echo "[ERR] PROD_READ_REGISTRY_* secrets are not configured" >&2
            exit 1
          fi
          if [ -z "$dev_user" ] || [ -z "$dev_pass" ]; then
            echo "[ERR] BOOTSTRAP_DEV_REGISTRY_* secrets are not configured" >&2
            exit 1
          fi
          echo "::add-mask::$prod_user"
          echo "::add-mask::$prod_pass"
          echo "::add-mask::$dev_user"
          echo "::add-mask::$dev_pass"
          prod_auth_b64=$(printf '%s:%s' "$prod_user" "$prod_pass" | base64 | tr -d '\n')
          dev_auth_b64=$(printf '%s:%s' "$dev_user" "$dev_pass" | base64 | tr -d '\n')
          docker_cfg=$(printf '{"auths":{"registry.deckhouse.io":{"auth":"%s"},"dev-registry.deckhouse.io":{"auth":"%s"}}}' "$prod_auth_b64" "$dev_auth_b64")
          docker_cfg_b64=$(printf '%s' "$docker_cfg" | base64 | tr -d '\n')
          echo "::add-mask::$docker_cfg_b64"
          {
            echo "REGISTRY_DOCKER_CFG=$docker_cfg_b64"
            echo "DECKHOUSE_REGISTRY_USER=$prod_user"
            echo "DECKHOUSE_REGISTRY_PASSWORD=$prod_pass"
          } >> "$GITHUB_ENV"

      - name: Configure registry auth (REGISTRY_DOCKER_CFG)
        working-directory: ci/dvp-e2e
        run: |
          yq eval --inplace '.deckhouse.registryDockerCfg = strenv(REGISTRY_DOCKER_CFG)' "${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml"

      - name: Docker login to Deckhouse registry
        uses: docker/login-action@v3
        with:
          registry: registry.deckhouse.io
          username: ${{ env.DECKHOUSE_REGISTRY_USER }}
          password: ${{ env.DECKHOUSE_REGISTRY_PASSWORD }}

      - name: Docker login to Deckhouse dev registry
        uses: docker/login-action@v3
        with:
          registry: ${{ vars.DEV_REGISTRY }}
          username: ${{ secrets.BOOTSTRAP_DEV_REGISTRY_LOGIN }}
          password: ${{ secrets.BOOTSTRAP_DEV_REGISTRY_PASSWORD }}

      - name: Configure storage profile
        working-directory: ci/dvp-e2e
        id: profile-config
        env:
          PROFILE: ${{ matrix.profile }}
        run: |
          # Get storage class configuration from profiles.json
          PROFILE_CONFIG=$(./scripts/get_profile_config.sh "${PROFILE}")

          # Parse the output more carefully
          STORAGE_CLASS=$(echo "$PROFILE_CONFIG" | grep "^STORAGE_CLASS=" | cut -d'=' -f2)
          IMAGE_STORAGE_CLASS=$(echo "$PROFILE_CONFIG" | grep "^IMAGE_STORAGE_CLASS=" | cut -d'=' -f2)
          SNAPSHOT_STORAGE_CLASS=$(echo "$PROFILE_CONFIG" | grep "^SNAPSHOT_STORAGE_CLASS=" | cut -d'=' -f2)
          ATTACH_DISK_SIZE=$(echo "$PROFILE_CONFIG" | grep "^ATTACH_DISK_SIZE=" | cut -d'=' -f2)

          echo "Profile: ${PROFILE}"
          echo "Storage Class: ${STORAGE_CLASS}"
          echo "Image Storage Class: ${IMAGE_STORAGE_CLASS}"
          echo "Snapshot Storage Class: ${SNAPSHOT_STORAGE_CLASS}"
          echo "Attach Disk Size: ${ATTACH_DISK_SIZE}"

          # Export variables safely
          echo "STORAGE_CLASS=${STORAGE_CLASS}" >> $GITHUB_ENV
          echo "IMAGE_STORAGE_CLASS=${IMAGE_STORAGE_CLASS}" >> $GITHUB_ENV
          echo "SNAPSHOT_STORAGE_CLASS=${SNAPSHOT_STORAGE_CLASS}" >> $GITHUB_ENV
          echo "ATTACH_DISK_SIZE=${ATTACH_DISK_SIZE}" >> $GITHUB_ENV
          echo "storage_class=$STORAGE_CLASS" >> $GITHUB_OUTPUT
          echo "image_storage_class=$IMAGE_STORAGE_CLASS" >> $GITHUB_OUTPUT
          echo "snapshot_storage_class=$SNAPSHOT_STORAGE_CLASS" >> $GITHUB_OUTPUT
          echo "attach_disk_size=$ATTACH_DISK_SIZE" >> $GITHUB_OUTPUT
          # Pass storage profile into run values for Helm templates
          PROFILE='${{ matrix.profile }}' yq eval --inplace '.storageProfile = strenv(PROFILE)' "${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml"

      - name: Install infra (namespace/RBAC/jump-host)
        working-directory: ci/dvp-e2e
        run: |
          USE_GH_SSH_KEYS=true SSH_FILE_NAME=id_ed task render-infra \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            PARENT_KUBECONFIG="${KUBECONFIG}" \
            SSH_FILE_NAME="id_ed"
          USE_GH_SSH_KEYS=true SSH_FILE_NAME=id_ed task infra-deploy \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            PARENT_KUBECONFIG="${KUBECONFIG}" \
            SSH_FILE_NAME="id_ed"

      - name: Bootstrap nested cluster
        working-directory: ci/dvp-e2e
        run: |
          echo "ðŸš€ dhctl bootstrap (profile: ${{ matrix.profile }})"
          task dhctl-bootstrap \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            PARENT_KUBECONFIG="${KUBECONFIG}" \
            SSH_FILE_NAME="id_ed" \
            TARGET_STORAGE_CLASS="ceph-pool-r2-csi-rbd-immediate"

      - name: Attach data disks to worker VMs using hotplug
        working-directory: ci/dvp-e2e
        run: |
          task infra:attach-storage-disks-hotplug \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            PARENT_KUBECONFIG="${KUBECONFIG}" \
            DISK_SIZE="${ATTACH_DISK_SIZE:-10Gi}" \
            STORAGE_CLASS="ceph-pool-r2-csi-rbd-immediate" \
            DISK_COUNT="2"

      - name: Build nested kubeconfig
        working-directory: ci/dvp-e2e
        run: |
          task nested:kubeconfig \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            NAMESPACE="${{ env.RUN_ID }}" \
            SSH_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/ssh" \
            SSH_FILE_NAME="id_ed" \
            NESTED_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested" \
            NESTED_KUBECONFIG="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested/kubeconfig" \
            PARENT_KUBECONFIG="${KUBECONFIG}"


      - name: Configure storage backend (post-bootstrap)
        run: |
          set -euo pipefail
          NESTED_KUBECONFIG="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested/kubeconfig"
          PROFILE="${{ matrix.profile }}"

          echo "[INFO] Waiting for nested API Server readiness..."
          for i in $(seq 1 30); do
            if KUBECONFIG="$NESTED_KUBECONFIG" kubectl get nodes >/dev/null 2>&1; then
              echo "[INFO] API Server is ready"; break
            fi
            echo "[INFO] Waiting for API Server... $i/30"; sleep 10
          done

          

          # Configure deckhouse-prod ModuleSource if Ceph profile is requested
          if [ "$PROFILE" = "cephrbd" ]; then
            echo "[Deckhouse] Configuring ModuleSource 'deckhouse-prod' for module images..."
            MODSRC_REPO="${{ vars.DECKHOUSE_PROD_MODULES_REPO || 'registry.deckhouse.io/deckhouse/ee/modules' }}"
            CFG_B64="${{ env.REGISTRY_DOCKER_CFG || '' }}"
            if [ -z "$CFG_B64" ]; then
              echo "[ERR] REGISTRY_DOCKER_CFG is empty; ensure PROD_READ_REGISTRY_* secrets are configured." >&2
              exit 1
            else
            success=0
            for attempt in $(seq 1 5); do
              if REPO="$MODSRC_REPO" CFG="$CFG_B64" yq eval -n '
                    .apiVersion = "deckhouse.io/v1alpha1" |
                    .kind = "ModuleSource" |
                    .metadata.name = "deckhouse-prod" |
                    .spec.registry.repo = env(REPO) |
                    .spec.registry.scheme = "HTTPS" |
                    .spec.registry.dockerCfg = env(CFG) |
                    .spec.releaseChannel = "EarlyAccess"
                  ' | KUBECONFIG="$NESTED_KUBECONFIG" kubectl apply --validate=false -f -; then
                success=1
                break
              fi
              echo "[Deckhouse] ModuleSource apply failed (attempt $attempt); retry in 10s..."
              sleep 10
            done
            if [ "$success" -ne 1 ]; then
              echo "[ERR] Failed to apply deckhouse-prod ModuleSource after retries" >&2
              exit 1
            fi
              echo "[Deckhouse] ModuleSource 'deckhouse-prod' applied."
              KUBECONFIG="$NESTED_KUBECONFIG" kubectl get modulesources.deckhouse.io || true
            fi
          fi

          # If Ceph profile requested, enable Ceph modules and apply CephCluster
          echo "[Ceph] ModuleSource is configured; deferring Ceph setup to Taskfile..."
          if false && [ "$PROFILE" = "cephrbd" ]; then
            echo "[Ceph] Enabling operator-ceph and CSI-Ceph via ModuleConfig..."
            for i in {1..12}; do
              if KUBECONFIG="$NESTED_KUBECONFIG" kubectl --request-timeout=10s apply --validate=false -f ci/dvp-e2e/manifests/storage/operator-ceph.yaml; then
                break; else echo "[Ceph] operator-ceph apply failed, retry $i/12"; sleep 10; fi
            done
            echo "[Ceph] Waiting for rook-ceph-operator deployment..."
            for i in $(seq 1 60); do
              if KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get deploy rook-ceph-operator >/dev/null 2>&1; then
                KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph rollout status deploy/rook-ceph-operator --timeout=180s && break
              fi
              echo "[Ceph] Waiting for rook-ceph-operator (attempt $i/60)"; sleep 10
            done
            echo "[Ceph] Waiting for CephCluster CRD..."
            KUBECONFIG="$NESTED_KUBECONFIG" kubectl wait --for=condition=Established --timeout=300s crd/cephclusters.ceph.rook.io || true
            echo "[Ceph] Applying CephCluster CR (first, only CephCluster)"
            for i in {1..12}; do
              if yq 'select(.kind == "CephCluster")' ci/dvp-e2e/manifests/storage/ceph.yaml | \
                 KUBECONFIG="$NESTED_KUBECONFIG" kubectl --request-timeout=15s apply --validate=false -f -; then
                break; else echo "[Ceph] CephCluster apply failed, retry $i/12"; sleep 10; fi
            done

            echo "[Ceph] Waiting for CephCluster phase=Ready and HEALTH_OK..."
            for i in $(seq 1 60); do
              PHASE=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get cephcluster rook-ceph-cluster -o jsonpath='{.status.phase}' 2>/dev/null || true)
              HEALTH=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get cephcluster rook-ceph-cluster -o jsonpath='{.status.ceph.health}' 2>/dev/null || true)
              echo "[Ceph] Status: phase=${PHASE:-?} health=${HEALTH:-?}"
              if [ "$PHASE" = "Ready" ] && [ "$HEALTH" = "HEALTH_OK" ]; then
                break
              fi
              sleep 20
            done

            echo "[Ceph] Getting cluster connection details..."
            for i in $(seq 1 30); do
              FSID=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph exec deploy/rook-ceph-tools -c ceph-tools -- ceph fsid 2>/dev/null | tr -d '\r\n' || true)
              if [ -n "$FSID" ]; then break; fi
              echo "[Ceph] Waiting for FSID (attempt $i/30)..."; sleep 5
            done
            if [ -z "$FSID" ]; then
              echo "[ERR] Failed to get Ceph FSID" >&2
              exit 1
            fi
            echo "[Ceph] FSID: $FSID"

            USER_KEY=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph exec deploy/rook-ceph-tools -c ceph-tools -- ceph auth get-key client.admin 2>/dev/null | tr -d '\r\n' || true)
            if [ -z "$USER_KEY" ]; then
              echo "[ERR] Failed to get admin key" >&2
              exit 1
            fi
            echo "[Ceph] Admin key obtained"

            MON_IPS=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get svc -l ceph-mon -o jsonpath='{.items[*].spec.clusterIP}' 2>/dev/null || true)
            if [ -z "$MON_IPS" ]; then
              echo "[ERR] Failed to get monitor IPs" >&2
              exit 1
            fi
            echo "[Ceph] Monitors: $MON_IPS"

            echo "[Ceph] Creating CephClusterConnection..."
            MONITORS_YAML=$(echo "$MON_IPS" | tr ' ' '\n' | sed 's/^/    - /;s/$/:6789/')
            {
              echo "apiVersion: storage.deckhouse.io/v1alpha1"
              echo "kind: CephClusterConnection"
              echo "metadata:"
              echo "  name: ceph-cluster-1"
              echo "spec:"
              echo "  clusterID: ${FSID}"
              echo "  monitors:"
              echo "$MONITORS_YAML"
              echo "  userID: admin"
              echo "  userKey: ${USER_KEY}"
            } | KUBECONFIG="$NESTED_KUBECONFIG" kubectl apply -f -

            echo "[Ceph] Waiting for CephClusterConnection to be Created..."
            for i in $(seq 1 30); do
              PHASE=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl get cephclusterconnection ceph-cluster-1 -o jsonpath='{.status.phase}' 2>/dev/null || true)
              if [ "$PHASE" = "Created" ]; then break; fi
              echo "[Ceph] CephClusterConnection phase=$PHASE, retry $i/30"; sleep 5
            done

            echo "[Ceph] Disabling cloud-provider-dvp module to avoid StorageClass conflict..."
            yq eval -n '
              .apiVersion = "deckhouse.io/v1alpha1" |
              .kind = "ModuleConfig" |
              .metadata.name = "cloud-provider-dvp" |
              .spec.enabled = false
            ' | KUBECONFIG="$NESTED_KUBECONFIG" kubectl apply -f -
            sleep 10

            echo "[Ceph] Ensure required ServiceAccounts and imagePullSecrets (fallback)"
            # Create SAs if missing
            for sa in rook-ceph-cmd-reporter rook-ceph-default rook-ceph-mgr rook-ceph-osd; do
              KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph create sa "$sa" --dry-run=client -o yaml | KUBECONFIG="$NESTED_KUBECONFIG" kubectl apply -f - || true
            done
            PULL_SECRET=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get deploy rook-ceph-operator -o jsonpath='{.spec.template.spec.imagePullSecrets[0].name}' 2>/dev/null || true)
            if [ -n "$PULL_SECRET" ]; then
              echo "[Ceph] Propagating imagePullSecret '$PULL_SECRET' to SAs..."
              for sa in default rook-ceph-mgr rook-ceph-osd; do
                KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph patch sa "$sa" -p '{"imagePullSecrets":[{"name":"'$PULL_SECRET'"}]}' || true
              done
            else
              echo "[WARN] rook-ceph-operator has no imagePullSecrets; skipping SA patch"
            fi

            echo "[Ceph] Applying CephBlockPool (StorageClass will be created by CephStorageClass)..."
            yq 'select(.kind == "CephBlockPool")' ci/dvp-e2e/manifests/storage/ceph.yaml | \
              KUBECONFIG="$NESTED_KUBECONFIG" kubectl --request-timeout=15s apply --validate=false -f -

            echo "[Ceph] Waiting for CephBlockPool to be Ready..."
            for i in $(seq 1 30); do
              BP_PHASE=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get cephblockpool pool-rbd-auto-test -o jsonpath='{.status.phase}' 2>/dev/null || true)
              [ "$BP_PHASE" = "Ready" ] && break
              echo "[Ceph] BlockPool phase=$BP_PHASE; retry $i/30"; sleep 10
            done

            echo "[Ceph] Creating CephStorageClass..."
            yq eval -n '
              .apiVersion = "storage.deckhouse.io/v1alpha1" |
              .kind = "CephStorageClass" |
              .metadata.name = "ceph-pool-r2-csi-rbd-immediate" |
              .spec.clusterConnectionName = "ceph-cluster-1" |
              .spec.reclaimPolicy = "Delete" |
              .spec.type = "RBD" |
              .spec.rbd.defaultFSType = "ext4" |
              .spec.rbd.pool = "pool-rbd-auto-test"
            ' | KUBECONFIG="$NESTED_KUBECONFIG" kubectl apply -f -

            echo "[Ceph] Waiting for CephStorageClass to be Created..."
            for i in $(seq 1 60); do
              PHASE=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl get cephstorageclass ceph-pool-r2-csi-rbd-immediate -o jsonpath='{.status.phase}' 2>/dev/null || true)
              if [ "$PHASE" = "Created" ]; then
                echo "[Ceph] CephStorageClass is Created"; break
              fi
              echo "[Ceph] CephStorageClass phase=$PHASE, retry $i/60"; sleep 10
            done
            echo "[Ceph] Waiting for StorageClass to be created by CephStorageClass..."
            for i in $(seq 1 30); do
              PROV=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl get sc ceph-pool-r2-csi-rbd-immediate -o jsonpath='{.provisioner}' 2>/dev/null || true)
              if [ "$PROV" = "rbd.csi.ceph.com" ]; then
                echo "[Ceph] StorageClass created with correct provisioner"; break
              fi
              echo "[Ceph] Waiting for StorageClass, retry $i/30"; sleep 5
            done

            echo "[Ceph] Setting ceph-pool-r2-csi-rbd-immediate as default StorageClass..."
            DEFAULT_STORAGE_CLASS="ceph-pool-r2-csi-rbd-immediate"
            KUBECONFIG="$NESTED_KUBECONFIG" kubectl patch mc global --type='json' -p='[
              {
                "op": "replace",
                "path": "/spec/settings/defaultClusterStorageClass",
                "value": "'$DEFAULT_STORAGE_CLASS'"
              }
            ]'
          fi

      - name: Configure storage classes
        working-directory: ci/dvp-e2e
        run: |
          echo "ðŸ’¾ Configuring storage classes for profile: ${{ matrix.profile }}"
          if [ -z "${{ matrix.profile }}" ]; then
            echo "[ERR] matrix.profile is empty; aborting storage configuration" >&2
            exit 1
          fi
          task nested:storage:configure \
            STORAGE_PROFILE="${{ matrix.profile }}" \
            TARGET_STORAGE_CLASS="ceph-pool-r2-csi-rbd-immediate" \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            GENERATED_VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/generated-values.yaml" \
            SSH_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/ssh" \
            SSH_FILE_NAME="id_ed" \
            PASSWORD_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/password.txt" \
            PASSWORD_HASH_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/password-hash.txt" \
            NAMESPACE="${{ env.RUN_ID }}" \
            DOMAIN="" \
            DEFAULT_USER="ubuntu" \
            NESTED_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested" \
            NESTED_KUBECONFIG="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested/kubeconfig"

      - name: Enable ingress-nginx in nested cluster
        run: |
          set -euo pipefail
          NESTED_KUBECONFIG="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested/kubeconfig"
          echo "[Ingress] Enabling ModuleConfig ingress-nginx"
          cat <<'EOF' | KUBECONFIG="$NESTED_KUBECONFIG" kubectl apply -f -
          apiVersion: deckhouse.io/v1alpha1
          kind: ModuleConfig
          metadata:
            name: ingress-nginx
          spec:
            enabled: true
EOF

          echo "[Ingress] Creating IngressNginxController (HostPort, class=nginx)"
          cat <<'EOF' | KUBECONFIG="$NESTED_KUBECONFIG" kubectl apply -f -
          apiVersion: deckhouse.io/v1
          kind: IngressNginxController
          metadata:
            name: main
          spec:
            ingressClass: "nginx"
            inlet: "HostPort"
EOF

          echo "[Ingress] Waiting controller pod"
          for i in $(seq 1 60); do
            READY=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-ingress-nginx get pods -o jsonpath='{range .items[*]}{.metadata.name}{" "}{.status.phase}{"\n"}{end}' 2>/dev/null | awk '/^controller-/{print $2}' | head -n1)
            [ "$READY" = "Running" ] && { echo "[Ingress] Controller Running"; break; } || true
            echo "[Ingress] Waiting... $i/60"; sleep 5
          done

      - name: Ingress publish smoke (in-cluster)
        run: |
          set -euo pipefail
          NESTED_KUBECONFIG="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested/kubeconfig"
          RUN_NS="${{ env.RUN_ID }}-ingress"

          echo "[Ingress] Namespace $RUN_NS"
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl create ns "$RUN_NS" || true

          echo "[Ingress] Echo pod and Service"
          cat <<'EOF' | KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n "$RUN_NS" apply -f -
          apiVersion: v1
          kind: Pod
          metadata:
            name: echo
            labels:
              app: vm-http
          spec:
            containers:
            - name: echo
              image: hashicorp/http-echo:0.2.3
              args: ["-text=hello from ingress"]
              ports:
              - containerPort: 5678
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: vm-http
          spec:
            selector:
              app: vm-http
            ports:
            - name: http
              port: 80
              targetPort: 5678
EOF

          echo "[Ingress] Ingress resource"
          cat <<'EOF' | KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n "$RUN_NS" apply -f -
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: vm-http
            annotations:
              nginx.ingress.kubernetes.io/ssl-redirect: "false"
          spec:
            ingressClassName: nginx
            rules:
            - host: vm-http.localtest
              http:
                paths:
                - path: /
                  pathType: Prefix
                  backend:
                    service:
                      name: vm-http
                      port:
                        number: 80
EOF

          echo "[Ingress] Resolve controller host IP"
          HOST_IP=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-ingress-nginx get pods -o jsonpath='{range .items[*]}{.metadata.name}{" "}{.status.hostIP}{"\n"}{end}' | awk '/^controller-/{print $2; exit}')
          if [ -z "$HOST_IP" ]; then
            HOST_IP=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
          fi
          echo "[Ingress] HOST_IP=$HOST_IP"

          echo "[Ingress] Curl from in-cluster"
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n "$RUN_NS" run curl --image=curlimages/curl:8.10.1 --restart=Never --rm -i -- /bin/sh -c "curl -sS -H 'Host: vm-http.localtest' http://$HOST_IP/ | head -n1" || {
            echo "[Ingress] Smoke failed" >&2; exit 1; }

      - name: Ceph CSI smoke check (PVC)
        if: ${{ matrix.profile == 'cephrbd' }}
        run: |
          set -euo pipefail
          NESTED_KUBECONFIG="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested/kubeconfig"
          echo "[Ceph] Current pods in d8-operator-ceph:"
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get pods -o wide || true
          echo "[Ceph] CSI resources:"
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get ds,deploy | sed -n '/csi\|rook/p' || true
          echo "[Ceph] StorageClasses:"
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl get sc || true

          SMOKE_NS="ceph-smoke-${{ env.RUN_ID }}"
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl create ns "$SMOKE_NS" || true
          yq eval -n '
            .apiVersion = "v1" |
            .kind = "PersistentVolumeClaim" |
            .metadata.name = "smoke-pvc" |
            .spec.accessModes = ["ReadWriteOnce"] |
            .spec.storageClassName = "ceph-pool-r2-csi-rbd-immediate" |
            .spec.resources.requests.storage = "1Gi"
          ' | KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n "$SMOKE_NS" apply -f -

          echo "[Ceph] Waiting for PVC to become Bound..."
          for i in $(seq 1 60); do
            PHASE=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n "$SMOKE_NS" get pvc smoke-pvc -o jsonpath='{.status.phase}' 2>/dev/null || true)
            if [ "$PHASE" = "Bound" ]; then echo "[Ceph] PVC is Bound"; break; fi
            if [ "$i" -eq 60 ]; then
              echo "[ERR] PVC did not reach Bound state" >&2
              echo "[DEBUG] Describe PVC:"; KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n "$SMOKE_NS" describe pvc smoke-pvc || true
              echo "[DEBUG] Events in namespace:"; KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n "$SMOKE_NS" get events --sort-by=.lastTimestamp | tail -n 50 || true
              echo "[DEBUG] CSI pods:"; KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get pods -o wide || true
              exit 1
            fi
            sleep 10
          done

          echo "[Ceph] Cleanup smoke resources"
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n "$SMOKE_NS" delete pvc smoke-pvc --wait=true --ignore-not-found=true || true
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl delete ns "$SMOKE_NS" --wait=false || true

      - name: Upload run context
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: run-context-${{ env.RUN_ID }}
          path: |
            ci/dvp-e2e/tmp/runs/${{ env.RUN_ID }}
            ci/dvp-e2e/tmp/shared
          if-no-files-found: warn

  run-e2e:
    name: E2E Tests (${{ matrix.profile }})
    needs: [setup, prepare]
    runs-on: ubuntu-latest
    timeout-minutes: 300
    concurrency:
      group: e2e-${{ github.ref }}-${{ matrix.profile }}
      cancel-in-progress: true
    strategy:
      fail-fast: false
      matrix:
        profile: ${{ fromJson(needs.setup.outputs.profiles) }}

    env:
      GO_VERSION: "1.24.6"
      TMP_ROOT: ${{ github.workspace }}/ci/dvp-e2e/tmp

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: Install Task
        uses: arduino/setup-task@v2
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Install Helm
        uses: azure/setup-helm@v4.3.0
        with:
          version: v3.17.2

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: Install Deckhouse CLI
        env:
          D8_VERSION: v0.13.2
        run: |
          set -euo pipefail
          echo "Installing d8 ${D8_VERSION}..."
          curl -fsSL -o d8.tgz "https://deckhouse.io/downloads/deckhouse-cli/${D8_VERSION}/d8-${D8_VERSION}-linux-amd64.tar.gz"
          tar -xzf d8.tgz linux-amd64/bin/d8
          mv linux-amd64/bin/d8 /usr/local/bin/d8
          chmod +x /usr/local/bin/d8
          rm -rf d8.tgz linux-amd64
          d8 --version

      - name: Install yq
        run: |
          echo "Installing yq..."
          curl -L -o /usr/local/bin/yq https://github.com/mikefarah/yq/releases/download/v4.44.1/yq_linux_amd64
          chmod +x /usr/local/bin/yq

      - name: Restore run context
        uses: actions/download-artifact@v4
        with:
          name: run-context-${{ needs.prepare.outputs.run_id }}
          path: .

      - name: Export run variables
        run: |
          echo "RUN_ID=${{ needs.prepare.outputs.run_id }}" >> "$GITHUB_ENV"
          echo "PROFILE=${{ matrix.profile }}" >> "$GITHUB_ENV"
          echo "STORAGE_CLASS=${{ needs.prepare.outputs.storage_class }}" >> "$GITHUB_ENV"
          echo "IMAGE_STORAGE_CLASS=${{ needs.prepare.outputs.image_storage_class }}" >> "$GITHUB_ENV"
          echo "SNAPSHOT_STORAGE_CLASS=${{ needs.prepare.outputs.snapshot_storage_class }}" >> "$GITHUB_ENV"

      - name: Build parent kubeconfig from secret (tests)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$HOME/.kube"
          cat > "$HOME/.kube/config" <<EOF
          apiVersion: v1
          kind: Config
          clusters:
          - cluster:
              server: ${E2E_K8S_URL}
              insecure-skip-tls-verify: true
            name: parent
          contexts:
          - context:
              cluster: parent
              user: sa
            name: parent
          current-context: parent
          users:
          - name: sa
            user:
              token: "${{ secrets.E2E_NESTED_SA_SECRET }}"
EOF
          chmod 600 "$HOME/.kube/config"
          echo "KUBECONFIG=$HOME/.kube/config" >> "$GITHUB_ENV"

      - name: Prepare JUnit directory
        run: |
          mkdir -p "${{ github.workspace }}/ci/dvp-e2e/artifacts/${{ env.RUN_ID }}"

      - name: Run E2E tests
        working-directory: ci/dvp-e2e
        run: |
          echo "ðŸ§ª Running E2E tests for profile: ${{ matrix.profile }}"
          task nested:e2e \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            GENERATED_VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/generated-values.yaml" \
            SSH_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/ssh" \
            SSH_FILE_NAME="id_ed" \
            PASSWORD_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/password.txt" \
            PASSWORD_HASH_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/password-hash.txt" \
            NAMESPACE="${{ env.RUN_ID }}" \
            DOMAIN="" \
            DEFAULT_USER="ubuntu" \
            PARENT_KUBECONFIG="${KUBECONFIG}" \
            STORAGE_PROFILE="${{ matrix.profile }}" \
            NESTED_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested" \
            NESTED_KUBECONFIG="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested/kubeconfig" \
            JUNIT_PATH="${{ github.workspace }}/ci/dvp-e2e/artifacts/${{ env.RUN_ID }}/junit.xml" \
            FOCUS="" \
            SKIP="" \
            LABELS="" \
            STORAGE_CLASS="${STORAGE_CLASS}" \
            IMAGE_STORAGE_CLASS="${IMAGE_STORAGE_CLASS}" \
            SNAPSHOT_STORAGE_CLASS="${SNAPSHOT_STORAGE_CLASS}" \
            TIMEOUT="${{ inputs.timeout || '4h' }}"

      - name: Collect JUnit for this run
        if: always()
        run: |
          JUNIT_OUT="${{ github.workspace }}/ci/dvp-e2e/artifacts/${{ env.RUN_ID }}/junit.xml"
          mkdir -p "$(dirname "$JUNIT_OUT")"
          if [ -f "$JUNIT_OUT" ]; then
            echo "JUnit file found at $JUNIT_OUT"
          else
            echo "junit.xml not found at expected location $JUNIT_OUT"
          fi

      - name: Collect matrix log
        if: always()
        run: |
          mkdir -p ci/dvp-e2e/tmp/matrix-logs
          LOG="ci/dvp-e2e/tmp/matrix-logs/${RUN_ID}.log"
          {
            echo "[START] run_id=${RUN_ID} time=$(date -Iseconds)"
            echo "[FINISH] run_id=${RUN_ID} status=ok time=$(date -Iseconds)"
          } >> "$LOG"
          echo "âœ… Created matrix log: $LOG"

      - name: Upload test logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-${{ env.RUN_ID }}
          path: ci/dvp-e2e/tmp/matrix-logs
          if-no-files-found: ignore

      - name: Upload JUnit report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: junit-${{ env.RUN_ID }}
          path: ci/dvp-e2e/artifacts
          if-no-files-found: ignore

      - name: Purge local artifacts and tmp
        if: always()
        run: |
          rm -rf ci/dvp-e2e/artifacts ci/dvp-e2e/tmp || true

  # ============================================
  # 3. REPORT - Result aggregation
  # ============================================
  report:
    name: Report Results
    needs: [setup, run-e2e]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: Build parent kubeconfig from secret (report)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$HOME/.kube"
          cat > "$HOME/.kube/config" <<EOF
          apiVersion: v1
          kind: Config
          clusters:
          - cluster:
              server: ${E2E_K8S_URL}
              insecure-skip-tls-verify: true
            name: parent
          contexts:
          - context:
              cluster: parent
              user: sa
            name: parent
          current-context: parent
          users:
          - name: sa
            user:
              token: "${{ secrets.E2E_NESTED_SA_SECRET }}"
EOF
          chmod 600 "$HOME/.kube/config"
          echo "KUBECONFIG=$HOME/.kube/config" >> "$GITHUB_ENV"

      - name: Download all JUnit reports
        uses: actions/download-artifact@v4
        with:
          pattern: junit-*
          path: ./results
          merge-multiple: true

      - name: Download matrix logs
        uses: actions/download-artifact@v4
        with:
          pattern: logs-*
          path: ci/dvp-e2e/tmp/matrix-logs
          merge-multiple: true

      - name: Generate matrix summary
        if: always()
        working-directory: ci/dvp-e2e
        run: |
          python3 scripts/loop_matrix_summary.py \
            --profiles "${{ join(needs.setup.outputs.profiles, ',') }}" \
            --run-id-prefix "nightly" \
            --log-dir "tmp/matrix-logs" \
            --webhook-url "${{ secrets.LOOP_WEBHOOK_URL || secrets.LOOP_WEBHOOK }}" \
            --channel "${{ secrets.LOOP_CHANNEL || 'test-virtualization-loop-alerts' }}" > matrix_summary.md || true
          DATE=$(date +"%Y-%m-%d")
          HASH=$(head -c 16 /dev/urandom | base64 | tr -dc 'a-z0-9' | head -c 8)
          kubectl apply -f - <<EOF || true
          apiVersion: v1
          kind: Secret
          metadata:
            name: "e2e-matrix-report-${DATE}-${HASH:0:8}"
            namespace: default
            labels:
              storageClass: "matrix"
          type: Opaque
          stringData:
            summary: |
              $(cat matrix_summary.md | sed 's/^/      /')
EOF

      - name: Create test summary
        if: always()
        run: |
          echo "### E2E Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Profile | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|--------|" >> $GITHUB_STEP_SUMMARY

          # Check each profile result
          for profile in sds cephrbd; do
            if compgen -G "./results/junit-${profile}-*/junit.xml" > /dev/null; then
              echo "| $profile | âœ… Completed |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| $profile | âŒ Failed/Missing |" >> $GITHUB_STEP_SUMMARY
            fi
          done

  # ============================================
  # 4. CLEANUP - Resource cleanup
  # ============================================
  cleanup:
    name: Cleanup Resources
    needs: report
    if: always()
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Task
        uses: arduino/setup-task@v2

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: Build parent kubeconfig from secret (cleanup)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$HOME/.kube"
          cat > "$HOME/.kube/config" <<EOF
          apiVersion: v1
          kind: Config
          clusters:
          - cluster:
              server: ${E2E_K8S_URL}
              insecure-skip-tls-verify: true
            name: parent
          contexts:
          - context:
              cluster: parent
              user: sa
            name: parent
          current-context: parent
          users:
          - name: sa
            user:
              token: "${{ secrets.E2E_NESTED_SA_SECRET }}"
EOF
          chmod 600 "$HOME/.kube/config"
          echo "KUBECONFIG=$HOME/.kube/config" >> "$GITHUB_ENV"

      - name: Cleanup test namespaces
        run: |
          set -euo pipefail
          PREFIX="nightly-nested-e2e-"
          echo "ðŸ§¹ Cleaning up namespaces matching prefix '${PREFIX}'"
          mapfile -t CANDIDATES < <(kubectl get ns -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | grep "^${PREFIX}" || true)
          OURS=()
          for ns in "${CANDIDATES[@]:-}"; do
            [ -z "$ns" ] && continue
            if kubectl -n "$ns" get deploy jump-host >/dev/null 2>&1; then
              OURS+=("$ns")
            fi
          done
          if [ "${#OURS[@]}" -eq 0 ]; then
            echo "[INFO] No namespaces to delete."
          else
            echo "[INFO] Deleting namespaces:"
            printf ' - %s\n' "${OURS[@]}"
            for ns in "${OURS[@]}"; do
              kubectl delete ns "$ns" --wait=false || true
            done
          fi

      - name: Cleanup ingress smoke namespaces
        if: always()
        run: |
          set -euo pipefail
          echo "[Cleanup] Deleting ingress smoke namespaces matching nightly-nested-e2e-*-ingress"
          kubectl get ns -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | awk '/^nightly-nested-e2e-.*-ingress$/{print $1}' | while read -r ns; do
            echo "[Cleanup] Deleting namespace $ns"; kubectl delete ns "$ns" --wait=false || true;
          done

      - name: Report cleanup results
        if: always()
        run: |
          echo "### Cleanup Results" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Cleanup job completed" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ§¹ Attempted to clean up namespaces matching 'nightly-nested-e2e-*'" >> $GITHUB_STEP_SUMMARY

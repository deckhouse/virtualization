# Copyright 2025 Flant JSC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: E2E Matrix Tests (DVP-over-DVP)

on:
  push:
    branches:
      - feat/ci-e2e-matrix
  pull_request:
    types: [opened, reopened, synchronize, labeled, unlabeled]
    branches:
      - main
      - feat/ci-e2e-matrix
  schedule:
    - cron: "30 2 * * *"
  workflow_dispatch:
    inputs:
      profiles:
        description: "Storage profiles (comma-separated): sds, cephrbd"
        required: false
        default: "sds,cephrbd"
      timeout:
        description: "Ginkgo timeout (e.g. 2h, 4h)"
        required: false
        default: "4h"

permissions:
  contents: read

env:
  E2E_K8S_URL: https://api.e2e.virtlab.flant.com

jobs:
  # ============================================
  # 1. SETUP - Environment preparation
  # ============================================
  setup:
    name: Setup Environment
    runs-on: ubuntu-latest
    outputs:
      profiles: ${{ steps.load.outputs.profiles }}
    steps:
      - uses: actions/checkout@v4

      - name: Load storage profiles
        id: load
        run: |
          # Load profiles dynamically from profiles.json
          cd ci/dvp-e2e
          PROFILES=$(jq -r '[.[].name] | @json' profiles.json)
          echo "profiles=$PROFILES" >> "$GITHUB_OUTPUT"

      - name: Print matrix
        run: |
          echo "Will test profiles: ${{ steps.load.outputs.profiles }}"

  # ============================================
  # 2. E2E - Parallel test execution
  # ============================================
  e2e:
    name: E2E (${{ matrix.profile }})
    needs: [setup]
    runs-on: ubuntu-latest
    timeout-minutes: 300
    concurrency:
      group: e2e-${{ github.ref }}-${{ matrix.profile }}
      cancel-in-progress: true
    strategy:
      fail-fast: false
      matrix:
        profile: ${{ fromJson(needs.setup.outputs.profiles) }}

    env:
      GO_VERSION: "1.24.6"
      TMP_ROOT: ${{ github.workspace }}/ci/dvp-e2e/tmp
      LOOP_WEBHOOK: ${{ secrets.LOOP_WEBHOOK_URL || secrets.LOOP_WEBHOOK }}
      LOOP_CHANNEL: ${{ secrets.LOOP_CHANNEL || 'test-virtualization-loop-alerts' }}  # TODO: replace with channel secret after successful run

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: Install Task
        uses: arduino/setup-task@v2
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Install Helm
        uses: azure/setup-helm@v4.3.0
        with:
          version: v3.17.2

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: Install Deckhouse CLI
        env:
          D8_VERSION: v0.13.2
        run: |
          set -euo pipefail
          echo "Installing d8 ${D8_VERSION}..."
          curl -fsSL -o d8.tgz "https://deckhouse.io/downloads/deckhouse-cli/${D8_VERSION}/d8-${D8_VERSION}-linux-amd64.tar.gz"
          tar -xzf d8.tgz linux-amd64/bin/d8
          mv linux-amd64/bin/d8 /usr/local/bin/d8
          chmod +x /usr/local/bin/d8
          rm -rf d8.tgz linux-amd64
          d8 --version

      - name: Install yq
        run: |
          echo "Installing yq..."
          curl -L -o /usr/local/bin/yq https://github.com/mikefarah/yq/releases/download/v4.44.1/yq_linux_amd64
          chmod +x /usr/local/bin/yq

      - name: Prepare environment
        id: prep
        run: |
          RUN_ID="nightly-nested-e2e-${{ matrix.profile }}-$(date +%H%M)"
          echo "run_id=$RUN_ID" >> "$GITHUB_OUTPUT"
          echo "RUN_ID=$RUN_ID" >> "$GITHUB_ENV"
          echo "PROFILE=${{ matrix.profile }}" >> "$GITHUB_ENV"
          echo "TMP_ROOT=${{ env.TMP_ROOT }}" >> "$GITHUB_ENV"
          mkdir -p "${{ env.TMP_ROOT }}/shared" "${{ env.TMP_ROOT }}/matrix-logs"

      - name: Build parent kubeconfig from secret
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$HOME/.kube"
          cat > "$HOME/.kube/config" <<EOF
          apiVersion: v1
          kind: Config
          clusters:
          - cluster:
              server: ${E2E_K8S_URL}
              insecure-skip-tls-verify: true
            name: parent
          contexts:
          - context:
              cluster: parent
              user: sa
            name: parent
          current-context: parent
          users:
          - name: sa
            user:
              token: "${{ secrets.E2E_NESTED_SA_SECRET }}"
          EOF
          chmod 600 "$HOME/.kube/config"
          echo "KUBECONFIG=$HOME/.kube/config" >> "$GITHUB_ENV"

      - name: Prepare run values.yaml
        working-directory: ci/dvp-e2e
        run: |
          task run:values:prepare \
            RUN_ID="${{ env.RUN_ID }}" \
            RUN_NAMESPACE="${{ env.RUN_ID }}" \
            RUN_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}"
          echo "VALUES_TEMPLATE_FILE=${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" >> $GITHUB_ENV

      - name: Configure registry auth (REGISTRY_DOCKER_CFG)
        working-directory: ci/dvp-e2e
        env:
          REGISTRY_DOCKER_CFG: ${{ secrets.REGISTRY_DOCKER_CFG }}
        run: |
          REGISTRY_DOCKER_CFG='${{ secrets.REGISTRY_DOCKER_CFG }}' yq eval --inplace '.deckhouse.registryDockerCfg = strenv(REGISTRY_DOCKER_CFG)' "${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml"

      - name: Docker login to Deckhouse dev registry
        uses: docker/login-action@v3
        with:
          registry: ${{ vars.DEV_REGISTRY }}
          username: ${{ secrets.BOOTSTRAP_DEV_REGISTRY_LOGIN }}
          password: ${{ secrets.BOOTSTRAP_DEV_REGISTRY_PASSWORD }}

      - name: Configure storage profile
        working-directory: ci/dvp-e2e
        id: profile-config
        env:
          PROFILE: ${{ matrix.profile }}
        run: |
          # Get storage class configuration from profiles.json
          PROFILE_CONFIG=$(./scripts/get_profile_config.sh "${PROFILE}")

          # Parse the output more carefully
          STORAGE_CLASS=$(echo "$PROFILE_CONFIG" | grep "^STORAGE_CLASS=" | cut -d'=' -f2)
          IMAGE_STORAGE_CLASS=$(echo "$PROFILE_CONFIG" | grep "^IMAGE_STORAGE_CLASS=" | cut -d'=' -f2)
          SNAPSHOT_STORAGE_CLASS=$(echo "$PROFILE_CONFIG" | grep "^SNAPSHOT_STORAGE_CLASS=" | cut -d'=' -f2)
          ATTACH_DISK_SIZE=$(echo "$PROFILE_CONFIG" | grep "^ATTACH_DISK_SIZE=" | cut -d'=' -f2)

          echo "Profile: ${PROFILE}"
          echo "Storage Class: ${STORAGE_CLASS}"
          echo "Image Storage Class: ${IMAGE_STORAGE_CLASS}"
          echo "Snapshot Storage Class: ${SNAPSHOT_STORAGE_CLASS}"
          echo "Attach Disk Size: ${ATTACH_DISK_SIZE}"

          # Export variables safely
          echo "STORAGE_CLASS=${STORAGE_CLASS}" >> $GITHUB_ENV
          echo "IMAGE_STORAGE_CLASS=${IMAGE_STORAGE_CLASS}" >> $GITHUB_ENV
          echo "SNAPSHOT_STORAGE_CLASS=${SNAPSHOT_STORAGE_CLASS}" >> $GITHUB_ENV
          echo "ATTACH_DISK_SIZE=${ATTACH_DISK_SIZE}" >> $GITHUB_ENV
          # Pass storage profile into run values for Helm templates
          PROFILE='${{ matrix.profile }}' yq eval --inplace '.storageProfile = strenv(PROFILE)' "${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml"

      - name: Install infra (namespace/RBAC/jump-host)
        working-directory: ci/dvp-e2e
        run: |
          USE_GH_SSH_KEYS=true SSH_FILE_NAME=cloud task render-infra \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            PARENT_KUBECONFIG="${KUBECONFIG}" \
            SSH_FILE_NAME="cloud"
          USE_GH_SSH_KEYS=true SSH_FILE_NAME=cloud task infra-deploy \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            PARENT_KUBECONFIG="${KUBECONFIG}" \
            SSH_FILE_NAME="cloud"

      - name: Bootstrap nested cluster
        working-directory: ci/dvp-e2e
        run: |
          echo "ðŸš€ dhctl bootstrap (profile: ${{ matrix.profile }})"
          task dhctl-bootstrap \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            PARENT_KUBECONFIG="${KUBECONFIG}" \
            SSH_FILE_NAME="cloud" \
            TARGET_STORAGE_CLASS="ceph-pool-r2-csi-rbd-immediate"

      - name: Attach data disks to worker VMs using hotplug
        working-directory: ci/dvp-e2e
        run: |
          task infra:attach-storage-disks-hotplug \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            PARENT_KUBECONFIG="${KUBECONFIG}" \
            DISK_SIZE="${ATTACH_DISK_SIZE:-10Gi}" \
            STORAGE_CLASS="ceph-pool-r2-csi-rbd-immediate" \
            DISK_COUNT="2"

      - name: Build nested kubeconfig
        working-directory: ci/dvp-e2e
        run: |
          task nested:kubeconfig \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            NAMESPACE="${{ env.RUN_ID }}" \
            SSH_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/ssh" \
            SSH_FILE_NAME="cloud" \
            NESTED_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested" \
            NESTED_KUBECONFIG="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested/kubeconfig" \
            PARENT_KUBECONFIG="${KUBECONFIG}"


      - name: Configure storage backend (post-bootstrap)
        run: |
          set -euo pipefail
          NESTED_KUBECONFIG="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested/kubeconfig"
          PROFILE="${{ matrix.profile }}"

          echo "[INFO] Waiting for nested API Server readiness..."
          for i in $(seq 1 30); do
            if KUBECONFIG="$NESTED_KUBECONFIG" kubectl get nodes >/dev/null 2>&1; then
              echo "[INFO] API Server is ready"; break
            fi
            echo "[INFO] Waiting for API Server... $i/30"; sleep 10
          done

          echo "[Deckhouse] Waiting for Deckhouse to become Available..."
          if ! KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-system rollout status deploy/deckhouse --timeout=10m; then
            echo "[Deckhouse] Not Available in time. Quick status:" >&2
            KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-system get pods -o wide || true
            echo "[Deckhouse] Recent logs (tail 100):"
            KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-system logs deploy/deckhouse --tail=100 2>/dev/null | sed -n '/module\|ceph\|webhook\|error\|failed/p' || true
            # Continue anyway; subsequent applies have retries.
          fi

          # Wait for Deckhouse service endpoints before queue diagnostics
          echo "[Deckhouse] Waiting for 'deckhouse' endpoints..."
          for i in $(seq 1 60); do
            EPS=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-system get ep deckhouse -o jsonpath='{.subsets[*].addresses[*].ip}' 2>/dev/null || true)
            if [ -n "$EPS" ]; then
              echo "[Deckhouse] Endpoints: $EPS"; break
            fi
            echo "[Deckhouse] Waiting endpoints... $i/60"; sleep 5
          done

          # Debug: Platform queue via runner-installed d8 using nested kubeconfig
          echo "[DEBUG] Platform queue diagnostics (runner d8)..."
          if command -v d8 >/dev/null 2>&1; then
            echo "[DEBUG] d8 path: $(command -v d8 || echo 'not found')"
            d8 --version 2>&1 || d8 version 2>&1 || true
            ok=0
            for i in $(seq 1 12); do
              if KUBECONFIG="$NESTED_KUBECONFIG" d8 platform queue list --output json 2>/dev/null; then
                ok=1; break
              fi
              echo "[DEBUG] queue not ready ($i/12); sleep 5"; sleep 5
            done
            [ "$ok" -eq 1 ] || echo "[DEBUG] Failed to get platform queue via d8"
          else
            echo "[DEBUG] d8 not found on runner; skipping platform queue"
          fi
          echo ""
          echo "[DEBUG] Recent Deckhouse events..."
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-system get events --sort-by='.lastTimestamp' --tail=20 2>/dev/null || true

          # Ensure webhook service object exists before applying ModuleConfigs
          if ! KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-system get svc deckhouse >/dev/null 2>&1; then
            echo "[Deckhouse] Service deckhouse not found yet; waiting a bit..."
            for i in $(seq 1 12); do
              KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-system get svc deckhouse >/dev/null 2>&1 && break || true
              sleep 5
            done
          fi
          # Wait for Deckhouse endpoints and give a small grace period for webhook init
          echo "[Deckhouse] Verifying 'deckhouse' endpoints before ModuleConfig apply..."
          for i in $(seq 1 60); do
            EPS=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-system get ep deckhouse -o jsonpath='{.subsets[*].addresses[*].ip}' 2>/dev/null || true)
            if [ -n "$EPS" ]; then
              echo "[Deckhouse] Endpoints ready: $EPS"; break
            fi
            echo "[Deckhouse] Waiting endpoints... $i/60"; sleep 5
          done
          echo "[Deckhouse] Extra grace period for webhook init..."; sleep 20

          # Configure deckhouse-prod ModuleSource if Ceph profile is requested
          if [ "$PROFILE" = "cephrbd" ]; then
            echo "[Deckhouse] Configuring ModuleSource 'deckhouse-prod' for module images..."
            MODSRC_REPO="${{ vars.DECKHOUSE_PROD_MODULES_REPO || 'registry.deckhouse.io/deckhouse/ee/modules' }}"
            CFG_B64="${{ secrets.REGISTRY_DOCKER_CFG || '' }}"
            if [ -z "$CFG_B64" ]; then
              echo "[ERR] REGISTRY_DOCKER_CFG secret is not set; Ceph requires deckhouse-prod modules source." >&2
              exit 1
            else
              yq eval -n \
                --arg repo "$MODSRC_REPO" \
                --arg cfg "$CFG_B64" '
                  .apiVersion = "deckhouse.io/v1alpha1" |
                  .kind = "ModuleSource" |
                  .metadata.name = "deckhouse-prod" |
                  .spec.registry.repo = $repo |
                  .spec.registry.scheme = "HTTPS" |
                  .spec.registry.dockerCfg = $cfg |
                  .spec.releaseChannel = "EarlyAccess"
                ' | KUBECONFIG="$NESTED_KUBECONFIG" kubectl apply -f -
              echo "[Deckhouse] ModuleSource 'deckhouse-prod' applied."
              KUBECONFIG="$NESTED_KUBECONFIG" kubectl get modulesources.deckhouse.io || true
            fi
          fi

          # If Ceph profile requested, enable Ceph modules and apply CephCluster
          if [ "$PROFILE" = "cephrbd" ]; then
            echo "[Ceph] Enabling operator-ceph and CSI-Ceph via ModuleConfig..."
            for i in {1..12}; do
              if KUBECONFIG="$NESTED_KUBECONFIG" kubectl --request-timeout=10s apply --validate=false -f ci/dvp-e2e/manifests/storage/operator-ceph.yaml; then
                break; else echo "[Ceph] operator-ceph apply failed, retry $i/12"; sleep 10; fi
            done
            echo "[Ceph] Waiting for rook-ceph-operator deployment..."
            for i in $(seq 1 60); do
              if KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get deploy rook-ceph-operator >/dev/null 2>&1; then
                KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph rollout status deploy/rook-ceph-operator --timeout=180s && break
              fi
              echo "[Ceph] Waiting for rook-ceph-operator (attempt $i/60)"; sleep 10
            done
            echo "[Ceph] Waiting for CephCluster CRD..."
            KUBECONFIG="$NESTED_KUBECONFIG" kubectl wait --for=condition=Established --timeout=300s crd/cephclusters.ceph.rook.io || true
            echo "[Ceph] Applying CephCluster CR (first, only CephCluster)"
            for i in {1..12}; do
              if yq 'select(.kind == "CephCluster")' ci/dvp-e2e/manifests/storage/ceph.yaml | \
                 KUBECONFIG="$NESTED_KUBECONFIG" kubectl --request-timeout=15s apply --validate=false -f -; then
                break; else echo "[Ceph] CephCluster apply failed, retry $i/12"; sleep 10; fi
            done

            echo "[Ceph] Waiting for CephCluster phase=Ready and HEALTH_OK..."
            for i in $(seq 1 60); do
              PHASE=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get cephcluster rook-ceph-cluster -o jsonpath='{.status.phase}' 2>/dev/null || true)
              HEALTH=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get cephcluster rook-ceph-cluster -o jsonpath='{.status.ceph.health}' 2>/dev/null || true)
              echo "[Ceph] Status: phase=${PHASE:-?} health=${HEALTH:-?}"
              if [ "$PHASE" = "Ready" ] && [ "$HEALTH" = "HEALTH_OK" ]; then
                break
              fi
              sleep 20
            done

            echo "[Ceph] Getting cluster connection details..."
            for i in $(seq 1 30); do
              FSID=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph exec deploy/rook-ceph-tools -c ceph-tools -- ceph fsid 2>/dev/null | tr -d '\r\n' || true)
              if [ -n "$FSID" ]; then break; fi
              echo "[Ceph] Waiting for FSID (attempt $i/30)..."; sleep 5
            done
            if [ -z "$FSID" ]; then
              echo "[ERR] Failed to get Ceph FSID" >&2
              exit 1
            fi
            echo "[Ceph] FSID: $FSID"

            USER_KEY=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph exec deploy/rook-ceph-tools -c ceph-tools -- ceph auth get-key client.admin 2>/dev/null | tr -d '\r\n' || true)
            if [ -z "$USER_KEY" ]; then
              echo "[ERR] Failed to get admin key" >&2
              exit 1
            fi
            echo "[Ceph] Admin key obtained"

            MON_IPS=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get svc -l ceph-mon -o jsonpath='{.items[*].spec.clusterIP}' 2>/dev/null || true)
            if [ -z "$MON_IPS" ]; then
              echo "[ERR] Failed to get monitor IPs" >&2
              exit 1
            fi
            echo "[Ceph] Monitors: $MON_IPS"

            echo "[Ceph] Creating CephClusterConnection..."
            MONITORS_YAML=$(echo "$MON_IPS" | tr ' ' '\n' | sed 's/^/    - /;s/$/:6789/')
            {
              echo "apiVersion: storage.deckhouse.io/v1alpha1"
              echo "kind: CephClusterConnection"
              echo "metadata:"
              echo "  name: ceph-cluster-1"
              echo "spec:"
              echo "  clusterID: ${FSID}"
              echo "  monitors:"
              echo "$MONITORS_YAML"
              echo "  userID: admin"
              echo "  userKey: ${USER_KEY}"
            } | KUBECONFIG="$NESTED_KUBECONFIG" kubectl apply -f -

            echo "[Ceph] Waiting for CephClusterConnection to be Created..."
            for i in $(seq 1 30); do
              PHASE=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl get cephclusterconnection ceph-cluster-1 -o jsonpath='{.status.phase}' 2>/dev/null || true)
              if [ "$PHASE" = "Created" ]; then break; fi
              echo "[Ceph] CephClusterConnection phase=$PHASE, retry $i/30"; sleep 5
            done

            echo "[Ceph] Disabling cloud-provider-dvp module to avoid StorageClass conflict..."
            yq eval -n '
              .apiVersion = "deckhouse.io/v1alpha1" |
              .kind = "ModuleConfig" |
              .metadata.name = "cloud-provider-dvp" |
              .spec.enabled = false
            ' | KUBECONFIG="$NESTED_KUBECONFIG" kubectl apply -f -
            sleep 10

            echo "[Ceph] Ensure required ServiceAccounts and imagePullSecrets (fallback)"
            # Create SAs if missing
            for sa in rook-ceph-cmd-reporter rook-ceph-default rook-ceph-mgr rook-ceph-osd; do
              KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph create sa "$sa" --dry-run=client -o yaml | KUBECONFIG="$NESTED_KUBECONFIG" kubectl apply -f - || true
            done
            PULL_SECRET=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get deploy rook-ceph-operator -o jsonpath='{.spec.template.spec.imagePullSecrets[0].name}' 2>/dev/null || true)
            if [ -n "$PULL_SECRET" ]; then
              echo "[Ceph] Propagating imagePullSecret '$PULL_SECRET' to SAs..."
              for sa in default rook-ceph-mgr rook-ceph-osd; do
                KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph patch sa "$sa" -p '{"imagePullSecrets":[{"name":"'$PULL_SECRET'"}]}' || true
              done
            else
              echo "[WARN] rook-ceph-operator has no imagePullSecrets; skipping SA patch"
            fi

            echo "[Ceph] Applying CephBlockPool (StorageClass will be created by CephStorageClass)..."
            yq 'select(.kind == "CephBlockPool")' ci/dvp-e2e/manifests/storage/ceph.yaml | \
              KUBECONFIG="$NESTED_KUBECONFIG" kubectl --request-timeout=15s apply --validate=false -f -

            echo "[Ceph] Waiting for CephBlockPool to be Ready..."
            for i in $(seq 1 30); do
              BP_PHASE=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get cephblockpool pool-rbd-auto-test -o jsonpath='{.status.phase}' 2>/dev/null || true)
              [ "$BP_PHASE" = "Ready" ] && break
              echo "[Ceph] BlockPool phase=$BP_PHASE; retry $i/30"; sleep 10
            done

            echo "[Ceph] Creating CephStorageClass..."
            yq eval -n '
              .apiVersion = "storage.deckhouse.io/v1alpha1" |
              .kind = "CephStorageClass" |
              .metadata.name = "ceph-pool-r2-csi-rbd-immediate" |
              .spec.clusterConnectionName = "ceph-cluster-1" |
              .spec.reclaimPolicy = "Delete" |
              .spec.type = "RBD" |
              .spec.rbd.defaultFSType = "ext4" |
              .spec.rbd.pool = "pool-rbd-auto-test"
            ' | KUBECONFIG="$NESTED_KUBECONFIG" kubectl apply -f -

            echo "[Ceph] Waiting for CephStorageClass to be Created..."
            for i in $(seq 1 60); do
              PHASE=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl get cephstorageclass ceph-pool-r2-csi-rbd-immediate -o jsonpath='{.status.phase}' 2>/dev/null || true)
              if [ "$PHASE" = "Created" ]; then
                echo "[Ceph] CephStorageClass is Created"; break
              fi
              echo "[Ceph] CephStorageClass phase=$PHASE, retry $i/60"; sleep 10
            done
            echo "[Ceph] Waiting for StorageClass to be created by CephStorageClass..."
            for i in $(seq 1 30); do
              PROV=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl get sc ceph-pool-r2-csi-rbd-immediate -o jsonpath='{.provisioner}' 2>/dev/null || true)
              if [ "$PROV" = "rbd.csi.ceph.com" ]; then
                echo "[Ceph] StorageClass created with correct provisioner"; break
              fi
              echo "[Ceph] Waiting for StorageClass, retry $i/30"; sleep 5
            done
          fi

          echo "[Ceph] Setting ceph-pool-r2-csi-rbd-immediate as default StorageClass..."
          DEFAULT_STORAGE_CLASS="ceph-pool-r2-csi-rbd-immediate"
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl patch mc global --type='json' -p='[
            {
              "op": "replace",
              "path": "/spec/settings/defaultClusterStorageClass",
              "value": "'$DEFAULT_STORAGE_CLASS'"
            }
          ]'

      - name: Configure storage classes
        working-directory: ci/dvp-e2e
        run: |
          echo "ðŸ’¾ Configuring storage classes for profile: ${{ matrix.profile }}"
          if [ -z "${{ matrix.profile }}" ]; then
            echo "[ERR] matrix.profile is empty; aborting storage configuration" >&2
            exit 1
          fi
          task nested:storage:configure \
            STORAGE_PROFILE="${{ matrix.profile }}" \
            TARGET_STORAGE_CLASS="ceph-pool-r2-csi-rbd-immediate" \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            GENERATED_VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/generated-values.yaml" \
            SSH_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/ssh" \
            SSH_FILE_NAME="cloud" \
            PASSWORD_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/password.txt" \
            PASSWORD_HASH_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/password-hash.txt" \
            NAMESPACE="${{ env.RUN_ID }}" \
            DOMAIN="" \
            DEFAULT_USER="ubuntu" \
            NESTED_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested" \
            NESTED_KUBECONFIG="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested/kubeconfig"

      - name: Ceph CSI smoke check (PVC)
        if: ${{ matrix.profile == 'cephrbd' }}
        run: |
          set -euo pipefail
          NESTED_KUBECONFIG="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested/kubeconfig"
          echo "[Ceph] Current pods in d8-operator-ceph:"
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get pods -o wide || true
          echo "[Ceph] CSI resources:"
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get ds,deploy | sed -n '/csi\|rook/p' || true
          echo "[Ceph] StorageClasses:"
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl get sc || true

          SMOKE_NS="ceph-smoke-${{ env.RUN_ID }}"
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl create ns "$SMOKE_NS" || true
          yq eval -n '
            .apiVersion = "v1" |
            .kind = "PersistentVolumeClaim" |
            .metadata.name = "smoke-pvc" |
            .spec.accessModes = ["ReadWriteOnce"] |
            .spec.storageClassName = "ceph-pool-r2-csi-rbd-immediate" |
            .spec.resources.requests.storage = "1Gi"
          ' | KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n "$SMOKE_NS" apply -f -

          echo "[Ceph] Waiting for PVC to become Bound..."
          for i in $(seq 1 60); do
            PHASE=$(KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n "$SMOKE_NS" get pvc smoke-pvc -o jsonpath='{.status.phase}' 2>/dev/null || true)
            if [ "$PHASE" = "Bound" ]; then echo "[Ceph] PVC is Bound"; break; fi
            if [ "$i" -eq 60 ]; then
              echo "[ERR] PVC did not reach Bound state" >&2
              echo "[DEBUG] Describe PVC:"; KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n "$SMOKE_NS" describe pvc smoke-pvc || true
              echo "[DEBUG] Events in namespace:"; KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n "$SMOKE_NS" get events --sort-by=.lastTimestamp | tail -n 50 || true
              echo "[DEBUG] CSI pods:"; KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n d8-operator-ceph get pods -o wide || true
              exit 1
            fi
            sleep 10
          done

          echo "[Ceph] Cleanup smoke resources"
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl -n "$SMOKE_NS" delete pvc smoke-pvc --wait=true --ignore-not-found=true || true
          KUBECONFIG="$NESTED_KUBECONFIG" kubectl delete ns "$SMOKE_NS" --wait=false || true

      - name: Prepare JUnit directory
        run: |
          mkdir -p "${{ github.workspace }}/ci/dvp-e2e/artifacts/${{ env.RUN_ID }}"

      - name: Run E2E tests
        working-directory: ci/dvp-e2e
        run: |
          echo "ðŸ§ª Running E2E tests for profile: ${{ matrix.profile }}"
          task nested:e2e \
            TMP_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}" \
            VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/values.yaml" \
            GENERATED_VALUES_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/generated-values.yaml" \
            SSH_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/ssh" \
            SSH_FILE_NAME="cloud" \
            PASSWORD_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/password.txt" \
            PASSWORD_HASH_FILE="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/password-hash.txt" \
            NAMESPACE="${{ env.RUN_ID }}" \
            DOMAIN="" \
            DEFAULT_USER="ubuntu" \
            PARENT_KUBECONFIG="${KUBECONFIG}" \
            STORAGE_PROFILE="${{ matrix.profile }}" \
            NESTED_DIR="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested" \
            NESTED_KUBECONFIG="${{ env.TMP_ROOT }}/runs/${{ env.RUN_ID }}/nested/kubeconfig" \
            JUNIT_PATH="${{ github.workspace }}/ci/dvp-e2e/artifacts/${{ env.RUN_ID }}/junit.xml" \
            FOCUS="" \
            SKIP="" \
            LABELS="" \
            STORAGE_CLASS="${STORAGE_CLASS}" \
            IMAGE_STORAGE_CLASS="${IMAGE_STORAGE_CLASS}" \
            SNAPSHOT_STORAGE_CLASS="${SNAPSHOT_STORAGE_CLASS}" \
            TIMEOUT="${{ inputs.timeout || '4h' }}"

      - name: Collect JUnit for this run
        if: always()
        run: |
          JUNIT_OUT="${{ github.workspace }}/ci/dvp-e2e/artifacts/${{ env.RUN_ID }}/junit.xml"
          mkdir -p "$(dirname "$JUNIT_OUT")"
          # JUnit file should already be created by nested:test-run task
          if [ -f "$JUNIT_OUT" ]; then
            echo "JUnit file found at $JUNIT_OUT"
          else
            echo "junit.xml not found at expected location $JUNIT_OUT"
          fi

      - name: Collect matrix log
        if: always()
        run: |
          mkdir -p ci/dvp-e2e/tmp/matrix-logs
          LOG="ci/dvp-e2e/tmp/matrix-logs/${RUN_ID}.log"
          {
            echo "[START] run_id=${RUN_ID} time=$(date -Iseconds)"
            echo "[FINISH] run_id=${RUN_ID} status=ok time=$(date -Iseconds)"
          } >> "$LOG"
          echo "âœ… Created matrix log: $LOG"

      - name: Upload test logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-${{ env.RUN_ID }}
          path: ci/dvp-e2e/tmp/matrix-logs
          if-no-files-found: ignore

      - name: Upload JUnit report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: junit-${{ env.RUN_ID }}
          path: ci/dvp-e2e/artifacts
          if-no-files-found: ignore

      - name: Purge local artifacts and tmp
        if: always()
        run: |
          rm -rf ci/dvp-e2e/artifacts ci/dvp-e2e/tmp || true

  # ============================================
  # 3. REPORT - Result aggregation
  # ============================================
  report:
    name: Report Results
    needs: e2e
    if: always()
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: Build parent kubeconfig from secret (report)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$HOME/.kube"
          cat > "$HOME/.kube/config" <<EOF
          apiVersion: v1
          kind: Config
          clusters:
          - cluster:
              server: ${E2E_K8S_URL}
              insecure-skip-tls-verify: true
            name: parent
          contexts:
          - context:
              cluster: parent
              user: sa
            name: parent
          current-context: parent
          users:
          - name: sa
            user:
              token: "${{ secrets.E2E_NESTED_SA_SECRET }}"
          EOF
          chmod 600 "$HOME/.kube/config"
          echo "KUBECONFIG=$HOME/.kube/config" >> "$GITHUB_ENV"

      - name: Download all JUnit reports
        uses: actions/download-artifact@v4
        with:
          pattern: junit-*
          path: ./results
          merge-multiple: true

      - name: Download matrix logs
        uses: actions/download-artifact@v4
        with:
          pattern: logs-*
          path: ci/dvp-e2e/tmp/matrix-logs
          merge-multiple: true

      - name: Generate matrix summary
        if: always()
        working-directory: ci/dvp-e2e
        run: |
          python3 scripts/loop_matrix_summary.py \
            --profiles "${{ join(needs.setup.outputs.profiles, ',') }}" \
            --run-id-prefix "nightly" \
            --log-dir "tmp/matrix-logs" \
            --webhook-url "${{ secrets.LOOP_WEBHOOK_URL || secrets.LOOP_WEBHOOK }}" \
            --channel "${{ secrets.LOOP_CHANNEL || 'test-virtualization-loop-alerts' }}" > matrix_summary.md || true
          DATE=$(date +"%Y-%m-%d")
          HASH=$(head -c 16 /dev/urandom | base64 | tr -dc 'a-z0-9' | head -c 8)
          kubectl apply -f - <<EOF || true
          apiVersion: v1
          kind: Secret
          metadata:
            name: "e2e-matrix-report-${DATE}-${HASH:0:8}"
            namespace: default
            labels:
              storageClass: "matrix"
          type: Opaque
          stringData:
            summary: |
              $(cat matrix_summary.md | sed 's/^/      /')
          EOF

      - name: Create test summary
        if: always()
        run: |
          echo "### E2E Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Profile | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|--------|" >> $GITHUB_STEP_SUMMARY

          # Check each profile result
          for profile in sds cephrbd; do
            if compgen -G "./results/junit-${profile}-*/junit.xml" > /dev/null; then
              echo "| $profile | âœ… Completed |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| $profile | âŒ Failed/Missing |" >> $GITHUB_STEP_SUMMARY
            fi
          done

  # ============================================
  # 4. CLEANUP - Resource cleanup
  # ============================================
  cleanup:
    name: Cleanup Resources
    needs: report
    if: always()
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Task
        uses: arduino/setup-task@v2

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: Build parent kubeconfig from secret (cleanup)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$HOME/.kube"
          cat > "$HOME/.kube/config" <<EOF
          apiVersion: v1
          kind: Config
          clusters:
          - cluster:
              server: ${E2E_K8S_URL}
              insecure-skip-tls-verify: true
            name: parent
          contexts:
          - context:
              cluster: parent
              user: sa
            name: parent
          current-context: parent
          users:
          - name: sa
            user:
              token: "${{ secrets.E2E_NESTED_SA_SECRET }}"
          EOF
          chmod 600 "$HOME/.kube/config"
          echo "KUBECONFIG=$HOME/.kube/config" >> "$GITHUB_ENV"

      - name: Cleanup test namespaces
        working-directory: ci/dvp-e2e
        run: |
          echo "ðŸ§¹ Cleaning up test namespaces..."
          task cleanup:namespaces:safe \
            FILTER_PREFIX="nightly-nested-e2e-" \
            CONFIRM=true

      - name: Report cleanup results
        if: always()
        run: |
          echo "### Cleanup Results" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Cleanup job completed" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ§¹ Attempted to clean up namespaces matching 'nightly-nested-e2e-*'" >> $GITHUB_STEP_SUMMARY
